
# 线性链条件随机场

## 1.模型的设计与实现

    1. 适用于中文分词任务, 参考 CRF++ 的模板, 设计如下特征函数:


        1.1 状态特征, 一元特征(Unigram),  简化为与时间 t无关:

        ('U', pos, word, tag) =
        (type , offset , word=x[t+offset] , tag=y[t])

         x  y
        迈	B
        向	E  <-t
        充	B
        满	E

        索引标签
        index_tag: {0:'B', 1:'I', 2:'E', 3:'S'}

        模板中的偏移量集合
        U_feature_offset = [-2, -1, 0, 1, 2]

        eg.

        ('U', 0, '向', 2)
        'U': 特征类型为 一元 (Unigram);
         0 :  当前时刻 t  ;
       '向':  当前时刻 x[t]='向' ;
         2: y[t]=2, 当前时刻的标签, 2代表标签 'E'

        ('U', -1, '迈', 2)
        'U': 特征类型为 一元 (Unigram);
         -1 :  偏移量, 当前时刻 t 向前偏移1个时刻  ;
       '迈':  向前偏移1个时刻后的 x[t-1]='迈' ;
         2: y[t]=2, 当前时刻的标签, 2代表标签 'E'

        ('U', 1, '充', 2)
        'U': 特征类型为 一元 (Unigram);
         1 :  偏移量, 当前时刻 t 向后偏移1个时刻 ;
       '充': 向后偏移1个时刻后的 x[t+1]='充' ;
         2: y[t]=2, 当前时刻的标签, 2代表标签 'E'

         上述例子表明: t 时刻的隐藏状态 y[t] 不仅仅和 t时刻的观测 x[t]有关,
         还和 t-1 时刻的观测 x[t-1] 与 t+1 时刻的观测 x[t+1] 有关

         偏移量集合为[-2, -1, 0, 1, 2],表明每次要向前看2个和向后看2个, 相当于4-gram,
         对比HMM x[t] 只和 y[t] 有关, 模型的表现力变强了

        1.2 转移特征, 二元特征(Bigram), 简化为与时间 t 和 观测 x 无关

        ('B', pre_tag, tag)

        (type,pre_tag=y[t-1],tag=y[t])

        eg. ('B',0,2)
       'B': 特征类型为 二元 (Bigram);
         1: y[t-1]=0 前一个时刻为 状态0 ,0代表标签 'B'
         2: y[t]=2 当前时刻为 状态2 ,2代表标签 'E'

        1.3 特征的总数量为 Ucount×Vword×m + m×m

        (1) 状态特征个数为  Ucount×Vword×m
        其中 Ucount表示 U特征提取的位置数目，比如只提取 当前文字前1个，当前文字，当前文字后一个，则此时 Ucount=3，
        Vword为语料库中文字数量，m表示 标签数。

        (2) 转移特征个数为  m×m
         二元特征表达了 前一个标签到当前标签的转移概率, 标签个数为m , 一共有 mxm 种转移


    2.实现了学习算法:
      (1)带正则的梯度上升算法
      (2) 拟牛顿法 L-BGFS ( 调用 scipy.optimize)

    3.实现在已有的预训练的模型的基础上进行更进一步的训练(使用同一个数据集)

    4.实现了 基于维特比算法的 解码

    5.如果觉得 LinearCRF模型训练的慢, 可以读取 CRF++ 库训练得到的参数, 详见 类 CRFSegmentation


## 2.实验结果

(1)  使用 CRF++库 在PKU 数据集 训练 100轮, 并在微博的测试集上测试

     P = 0.71 R=0.72 F1=0.71



| Corpus | P | R | F1 |
| ------ | ---- | ---- | ---- |
| msr | 76.84263691915065 | 80.16949059509177 | 78.47081821412925 |
| pku | 89.23635340814322 | 88.86412962515287 | 89.04985254930146 |
| weibo | 71.06637402558087 | 72.00920245398773 | 71.53468175065706 |




# ref

1.《统计学习方法 第二版》李航

2. https://victorjiangxin.github.io/Chinese-Word-Segmentation/

3. https://www.cnblogs.com/Determined22/p/6915730.html
