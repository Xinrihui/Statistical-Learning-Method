# 线性模型

## 1.线性回归模型

### 1.1 模型设计

    1.实现了对数据的归一化

    由于 sklearn自带的波士顿房价数据集的各个维度的特征的数值范围差异较大,
    导致模型训练时会发生 loss 和 gradient 的上溢出现象, 因此对各个特征进行均值和方差的归一化

    另外, 标签y 和 特征X 的差距较大, 导致线性回归模型训练时不收敛, 解决方案:

     M1.可以对 y 进行归一化
     M2.在 h(x) 中加入偏置项 b, 一般线性回归都要考虑偏置项

    2.学习算法: 实现了带正则化的梯度下降 和 解析法直接求解模型的参数

### 1.2 实验结果

    波士顿房价数据集
    n_train = 452
    n_test = 100

    1. 不加入正则化 max_iter=50, learning_rate=0.1

        epcho: 0 , loss:295.76924175824155
        ...
        epcho: 49 , loss:11.68898429599343

        W: [-0.6352397   0.54656253 -0.33105276  0.63386307 -1.08895726  3.09167393
             -0.18907642 -2.18630367  1.0257064  -0.64927669 -1.91210443  0.85438756
             -3.45453258]

        by sklearn , the squared_error: 16.61168620800721
        by xrh , the squared_error: 16.21430292366462

    2. 加入L2正则化 reg_lambda=0.5,  max_iter=50, learning_rate=0.1

        epcho: 0 , loss:295.76924175824155
        ...
        epcho: 35 , loss:17.995616868685445
        ...
        epcho: 49 , loss:17.839313624370263

        W: [-0.54515783  0.41398274 -0.45347702  0.59311115 -0.59120612  2.44687475
             -0.23729531 -0.98859324  0.22114531 -0.51307395 -1.42225501  0.64833666
             -2.37634278]

        by xrh , the squared_error: 20.93320421835776

        我们发现在训练时损失在中间的 epcho就不再下降, 说明模型训练并没有出现过拟合, 反而是欠拟合的，
        为了提高在测试集上的表现, 可以调低 L2正则化 的强度

    3. 加入L2正则化 reg_lambda=0.1,  max_iter=50, learning_rate=0.1

       by xrh , the squared_error: 16.90898232586299


    4. 加入L1正则化 reg_alpha=0.5,  max_iter=50, learning_rate=0.1

       epcho: 0 , loss:295.76924175824155
       ...
       epcho: 28 , loss:18.92783812210059
       ...
       epcho: 49 , loss:18.14832577632172

       W: [-1.33345811e-01  7.00199249e-02 -3.48548579e-03  2.76635968e-01
             -1.16584281e-01  3.05385413e+00  3.11102299e-02 -3.12883736e-01
             -2.84816726e-03 -7.50169381e-02 -1.62881873e+00  4.95837410e-01
             -3.40253866e+00]

       by xrh , the squared_error: 20.34544600809011

       观察到特征的权重 W 中出现很小的值, 说明L1 正则化导致了特征选择的作用

    5. 对 y 归一化, 采用解析式直接求解

        by sklearn , the squared_error: 0.19677533221422852
        by xrh , the squared_error: 0.4146028764452961


## 2.逻辑回归模型


### 1.1 模型设计

    1. 计算梯度时, 所有样本的梯度求和后要取平均, 否则会造成溢出

    2. 学习算法: 实现了带正则化的梯度下降

    3. 可以返回样本的预测的分值, 后续可以利用此分值画出用于模型评价的 P-R曲线

### 1.2 实验结果

    Mnist 数据集(二分类)
    n_train = 60000
    n_test = 10000

    1. 加入L2正则化 reg_lambda=0.1,  max_iter=50, learning_rate=0.1

    training cost time : 107s
    accuracy: 0.9746

    Mnist 数据集(多分类)
    n_train = 60000
    n_test = 10000

    1. 加入L2正则化 reg_lambda=0.1,  max_iter=50, learning_rate=0.1

    training cost time : 30s
    accuracy:  0.84

    统计所有类别的 评价指标

                  precision    recall  f1-score   support

           0       0.89      0.96      0.92       980
           1       0.85      0.96      0.90      1135
           2       0.87      0.79      0.83      1032
           3       0.77      0.87      0.82      1010
           4       0.85      0.86      0.85       982
           5       0.89      0.60      0.72       892
           6       0.86      0.91      0.88       958
           7       0.88      0.86      0.87      1028
           8       0.80      0.79      0.80       974
           9       0.81      0.81      0.81      1009

    accuracy                           0.84     10000
   macro avg       0.85      0.84      0.84     10000
weighted avg       0.85      0.84      0.84     10000

    打印混淆矩阵

[[ 939    0    4    4    0    2   20    1   10    0]
 [   0 1090   11    3    0    1    4    0   26    0]
 [  20   39  818   31   23    0   30   24   45    2]
 [   5    5   20  882    1   22    9   19   33   14]
 [   3   13    6    3  844    0   19    2   11   81]
 [  31   32    8  146   31  532   34   17   40   21]
 [  23   10   15    2   13   16  874    0    5    0]
 [   7   46   25    1   14    0    4  880    9   42]
 [  12   30   16   60    9   15   20   15  771   26]
 [  18   17   14   16   59    8    4   46    9  818]]

