{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 获取 训练数据\n",
    "\n",
    "dataset = np.array(\n",
    "               [[1, 5, 20, 1.1],\n",
    "                [2, 7, 30, 1.3],\n",
    "                [3, 21, 70, 1.7],\n",
    "                [4, 30, 60, 1.8],\n",
    "               ])\n",
    "columns=['id', 'age', 'weight', 'label']\n",
    "\n",
    "\n",
    "X = dataset[:,0:3]\n",
    "y = dataset[:,3]\n",
    "\n",
    "np.mean(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import ensemble\n",
    " \n",
    " \n",
    "# 加载sklearn自带的波士顿房价数据集\n",
    "dataset = load_boston()\n",
    " \n",
    "# 提取特征数据和目标数据\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    " \n",
    "# 将数据集以9:1的比例随机分为训练集和测试集，为了重现随机分配设置随机种子，即random_state参数\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, test_size=0.1, random_state=188)\n",
    " \n",
    "# 实例化估计器对象\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "gbr = ensemble.GradientBoostingRegressor(**params)\n",
    " \n",
    "# 估计器拟合训练数据\n",
    "gbr.fit(X_train, y_train)\n",
    " \n",
    "# 训练完的估计器对测试数据进行预测\n",
    "y_pred = gbr.predict(X_test)\n",
    " \n",
    "# 输出特征重要性列表\n",
    "print(gbr.feature_importances_)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(  X):\n",
    "    \"\"\"\n",
    "    sigmoid 激活函数\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "sigmoid(float('-inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT 二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDBT_2Classifier 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('E:\\\\python package\\\\python-project\\\\统计学习方法\\\\Statistical-Learning-Method_Code-master\\\\DecisionTree')\n",
    "\n",
    "from CartTree_regression_xrh import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDBT_2Classifier:\n",
    "    \"\"\"\n",
    "\n",
    "    适用于 二分类 问题 的 梯度提升树\n",
    "\n",
    "    基分类器为 CART 回归树\n",
    "\n",
    "    Author: xrh\n",
    "    Date: 2021-04-10\n",
    "\n",
    "    ref: https://zhuanlan.zhihu.com/p/89549390\n",
    "\n",
    "\n",
    "    test1: 二分类任务\n",
    "\n",
    "    数据集：Mnist\n",
    "    参数: error_rate_threshold=0.01, max_iter=30, max_depth=3,learning_rate=0.2\n",
    "\n",
    "    训练集数量：60000\n",
    "    测试集数量：10000\n",
    "    正确率： 0.9891\n",
    "    模型训练时长：205.7052161693573\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, error_rate_threshold=0.05, max_iter=10, max_depth=1):\n",
    "        \"\"\"\n",
    "\n",
    "        :param error_rate_threshold: 训练中止条件, 若当前得到的基分类器的组合 的错误率 小于阈值, 则停止训练\n",
    "        :param max_iter: 最大迭代次数\n",
    "        :param max_depth: CART 回归树 的最大深度\n",
    "        \"\"\"\n",
    "\n",
    "        # 训练中止条件 error_rate  < self.error_rate_threshold ( 若当前得到的基分类器的组合 的错误率 小于阈值, 则停止训练)\n",
    "        self.error_rate_threshold = error_rate_threshold\n",
    "\n",
    "        # 最大迭代次数\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # CART 回归树 的最大深度\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.G = []  # 弱分类器 集合\n",
    "\n",
    "    def sigmoid( self , X ):\n",
    "        \"\"\"\n",
    "        sigmoid 激活函数\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def fit(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "\n",
    "        用 训练数据 拟合模型\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :param y: 标签数据 , shape=(N_sample,)\n",
    "        :param learning_rate: 学习率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        N = np.shape(X)[0]  # 样本的个数\n",
    "\n",
    "        f = 0  # 基分类器 的加权和\n",
    "\n",
    "        P_1 = len(y[y == 1]) / len(y)\n",
    "        y_predict = np.log(P_1 / (1 - P_1))\n",
    "\n",
    "        self.G.append(y_predict)\n",
    "\n",
    "        f += y_predict\n",
    "\n",
    "        feature_value_set = RegresionTree.get_feature_value_set(X)  # 可供选择的特征集合 , 包括 (特征, 切分值)\n",
    "\n",
    "        for m in range(self.max_iter):  # 进行 第 m 轮迭代\n",
    "\n",
    "            r = y - self.sigmoid(f)  # 残差\n",
    "\n",
    "            RT = RegresionTree_GBDT(min_square_loss=0.1, max_depth=self.max_depth,print_log=False)\n",
    "\n",
    "            RT.fit(X, r, y, feature_value_set=feature_value_set)\n",
    "\n",
    "            y_predict = RT.predict(X)\n",
    "\n",
    "            self.G.append((learning_rate, RT))  # 存储 基分类器\n",
    "\n",
    "            # 计算 当前 所有弱分类器加权 得到的 最终分类器 的 分类错误率\n",
    "\n",
    "            f += learning_rate * y_predict\n",
    "\n",
    "            G = self.sigmoid(f)\n",
    "\n",
    "            #TODO: 负例 设置为 -1 会导致 在训练集的 训练误差率无法下降, 原因未知\n",
    "\n",
    "            G[G >= 0.5] = 1  # 概率 大于 0.5 被标记为 正例\n",
    "            G[G < 0.5] = 0  # 概率 小于 0.5 被标记为 负例\n",
    "\n",
    "            err_arr = np.ones(N, dtype=int)\n",
    "            err_arr[G == y] = 0\n",
    "            err_rate = np.mean(err_arr)\n",
    "\n",
    "            print('round:{}, err_rate:{}'.format(m, err_rate))\n",
    "            print('======================')\n",
    "\n",
    "            if err_rate < self.error_rate_threshold:  # 错误率 已经小于 阈值, 则停止训练\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        对 测试 数据进行预测, 返回预测的标签\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        f = 0  # 最终分类器\n",
    "\n",
    "        f += self.G[0]  # 第一个 存储的是 初始化情况\n",
    "\n",
    "        for alpha, RT in self.G[1:]:\n",
    "            y_predict = RT.predict(X)\n",
    "            f += alpha * y_predict\n",
    "\n",
    "        # print('f:',f)\n",
    "\n",
    "        G = self.sigmoid(f)\n",
    "\n",
    "        # print('G:',G)\n",
    "\n",
    "        G[G >= 0.5] = 1  # 概率 大于 0.5 被标记为 正例\n",
    "        G[G < 0.5] = 0  # 概率 小于 0.5 被标记为 负例\n",
    "\n",
    "        return G\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        对 测试 数据进行预测, 返回预测的 概率值\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        f = 0  # 最终分类器\n",
    "\n",
    "        f += self.G[0]  # 第一个 存储的是 初始化情况\n",
    "\n",
    "        for alpha, RT in self.G[1:]:\n",
    "            y_predict = RT.predict(X)\n",
    "            f += alpha * y_predict\n",
    "\n",
    "        # print('f:',f)\n",
    "        G = self.sigmoid(f)\n",
    "\n",
    "        return G\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        使用 测试数据集 对模型进行评价, 返回正确率\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :param y: 标签数据 , shape=(N_sample,)\n",
    "        :return:  正确率 accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        N = np.shape(X)[0]  # 样本的个数\n",
    "\n",
    "        G = self.predict(X)\n",
    "\n",
    "        err_arr = np.ones(N, dtype=int)\n",
    "        err_arr[G == y] = 0\n",
    "        err_rate = np.mean(err_arr)\n",
    "\n",
    "        accuracy = 1 - err_rate\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData_2classification( fileName, n=1000):\n",
    "    '''\n",
    "    加载文件\n",
    "\n",
    "    将 数据集 的标签 转换为 二分类的标签\n",
    "\n",
    "    :param fileName:要加载的文件路径\n",
    "    :param n: 返回的数据集的规模\n",
    "    :return: 数据集和标签集\n",
    "    '''\n",
    "    # 存放数据及标记\n",
    "    dataArr = []\n",
    "    labelArr = []\n",
    "    # 读取文件\n",
    "    fr = open(fileName)\n",
    "\n",
    "    cnt = 0  # 计数器\n",
    "\n",
    "    # 遍历文件中的每一行\n",
    "    for line in fr.readlines():\n",
    "\n",
    "        if cnt == n:\n",
    "            break\n",
    "\n",
    "        # 获取当前行，并按“，”切割成字段放入列表中\n",
    "        # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "        # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "        curLine = line.strip().split(',')\n",
    "        # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "        # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "        # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "        dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "        # 将标记信息放入标记集中\n",
    "        # 转换成二分类任务\n",
    "        # 标签0设置为1，反之为0\n",
    "\n",
    "        # 显然这会导致 正负 样本的 分布不均衡, 1 的样本很少(10%), 而0 的很多\n",
    "        if int(curLine[0]) == 0:\n",
    "            labelArr.append(1)\n",
    "        else:\n",
    "            labelArr.append(0)\n",
    "\n",
    "        # if int(curLine[0]) <= 5:\n",
    "        #     labelArr.append(1)\n",
    "        # else:\n",
    "        #     labelArr.append(0)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    fr.close()\n",
    "\n",
    "    # 返回数据集和标记\n",
    "    return dataArr, labelArr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=6000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList =loadData_2classification('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 开始时间\n",
    "print('start training model....')\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "clf = GDBT_2Classifier( error_rate_threshold=0.01, max_iter=30, max_depth=3 )\n",
    "clf.fit(trainDataArr, trainLabelArr,learning_rate=0.2)\n",
    "\n",
    "\n",
    "# 结束时间\n",
    "end = time.time()\n",
    "print('training cost time :', end - start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf2 = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=30\n",
    "                                  , max_depth=3\n",
    "                                )\n",
    "clf2.fit(trainDataArr, trainLabelArr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test=1000\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData_2classification('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n",
    "\n",
    "print('test dataset accuracy: {} '.format(clf.score(testDataArr, testLabelArr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评估 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "y_pred= clf.predict( testDataArr )\n",
    "y_true=testLabelArr\n",
    "\n",
    "\n",
    "#0. 混淆矩阵\n",
    "\n",
    "\n",
    "print('confusion_matrix: ')\n",
    "print(confusion_matrix(y_true, y_pred)) # 主对角线上的 元素为 标签值 和 预测值 都为 第 i 类 的样本的数目\n",
    "\n",
    "# array([[ 83,   0,   0,   0,   0,   1,   1,   0,   0,   0],\n",
    "#        [  0, 124,   0,   1,   0,   0,   1,   0,   0,   0],\n",
    "#        [  0,   1, 104,   1,   0,   0,   1,   3,   5,   1],\n",
    "#        [  0,   0,   1,  96,   0,   5,   1,   1,   2,   1],\n",
    "#        [  1,   0,   0,   0, 100,   0,   1,   1,   1,   6],\n",
    "#        [  0,   0,   0,   3,   1,  79,   0,   3,   1,   0],\n",
    "#        [  3,   0,   0,   0,   1,   1,  82,   0,   0,   0],\n",
    "#        [  0,   0,   2,   2,   2,   1,   0,  87,   1,   4],\n",
    "#        [  0,   0,   1,   4,   3,   2,   0,   2,  77,   0],\n",
    "#        [  0,   0,   0,   0,   2,   0,   0,   3,   1,  88]], dtype=int64)\n",
    "\n",
    "# confusion_matrix[0][0]= 83  真实标签为 0 预测值也为0  的样本个数为 83 \n",
    "# confusion_matrix[2][8]= 5    真实标签为 2 预测值为8  的样本个数为 5\n",
    "\n",
    "#样本不均衡时,  改变样本的权重 \n",
    "sw = compute_sample_weight(class_weight='balanced',y=y_true)\n",
    "\n",
    "confusion_matrix(y_true, y_pred, sample_weight=sw)\n",
    "\n",
    "# array([[97.64705882,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "#          1.17647059,  1.17647059,  0.        ,  0.        ,  0.        ],\n",
    "#        [ 0.        , 98.41269841,  0.        ,  0.79365079,  0.        ,\n",
    "#          0.        ,  0.79365079,  0.        ,  0.        ,  0.        ],\n",
    "#        [ 0.        ,  0.86206897, 89.65517241,  0.86206897,  0.        ,\n",
    "#          0.        ,  0.86206897,  2.5862069 ,  4.31034483,  0.86206897],\n",
    "#        [ 0.        ,  0.        ,  0.93457944, 89.71962617,  0.        ,\n",
    "#          4.6728972 ,  0.93457944,  0.93457944,  1.86915888,  0.93457944],\n",
    "#        [ 0.90909091,  0.        ,  0.        ,  0.        , 90.90909091,\n",
    "#          0.        ,  0.90909091,  0.90909091,  0.90909091,  5.45454545],\n",
    "#        [ 0.        ,  0.        ,  0.        ,  3.44827586,  1.14942529,\n",
    "#         90.8045977 ,  0.        ,  3.44827586,  1.14942529,  0.        ],\n",
    "#        [ 3.44827586,  0.        ,  0.        ,  0.        ,  1.14942529,\n",
    "#          1.14942529, 94.25287356,  0.        ,  0.        ,  0.        ],\n",
    "#        [ 0.        ,  0.        ,  2.02020202,  2.02020202,  2.02020202,\n",
    "#          1.01010101,  0.        , 87.87878788,  1.01010101,  4.04040404],\n",
    "#        [ 0.        ,  0.        ,  1.12359551,  4.49438202,  3.37078652,\n",
    "#          2.24719101,  0.        ,  2.24719101, 86.51685393,  0.        ],\n",
    "#        [ 0.        ,  0.        ,  0.        ,  0.        ,  2.12765957,\n",
    "#          0.        ,  0.        ,  3.19148936,  1.06382979, 93.61702128]])\n",
    "\n",
    "\n",
    "print('====================')\n",
    "\n",
    "# 1.正确率\n",
    "print('test dataset accuracy: {} '.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "print('====================')\n",
    "\n",
    "# 2.精确率  \n",
    "\n",
    "# print(precision_score(y_true, y_pred, average='macro'))  # \n",
    "# print(precision_score(y_true, y_pred, average='micro'))  # \n",
    "# print(precision_score(y_true, y_pred, average='weighted'))  # \n",
    "\n",
    "print('pos-1 precision: ',precision_score(y_true, y_pred, average='binary')) \n",
    "\n",
    "precision_list= precision_score(y_true, y_pred, average=None)\n",
    "\n",
    "print('neg-0 precision:{}, pos-1 precision:{}  '.format(precision_list[0],precision_list[1]) ) \n",
    "\n",
    "print('====================')\n",
    "\n",
    "# 3. 召回率\n",
    "\n",
    "# print(recall_score(y_true, y_pred, average='macro'))  # \n",
    "# print(recall_score(y_true, y_pred, average='micro'))  # \n",
    "# print(recall_score(y_true, y_pred, average='weighted'))  # \n",
    "\n",
    "print('pos-1 recall: ',recall_score(y_true, y_pred, average='binary')) \n",
    "\n",
    "recall_list= recall_score(y_true, y_pred, average=None)\n",
    "\n",
    "print('neg-0 recall:{}, pos-1 recall:{}  '.format(recall_list[0],recall_list[1]) )\n",
    "\n",
    "print('====================')\n",
    "\n",
    "# 4. F1-score\n",
    "\n",
    "# print(f1_score(y_true, y_pred, average='macro'))  \n",
    "# print(f1_score(y_true, y_pred, average='micro')) \n",
    "# print(f1_score(y_true, y_pred, average='weighted'))  \n",
    "\n",
    "print('pos-1 f1_score: ',f1_score(y_true, y_pred, average='binary')) \n",
    "\n",
    "f1_score_list= f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "print('neg-0 f1_score:{}, pos-1 f1_score:{}  '.format(f1_score_list[0],f1_score_list[1]) )\n",
    "\n",
    "print('====================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores2= clf2.predict_proba(testDataArr)[:,0]\n",
    "# y_scores2\n",
    "\n",
    "precision2, recall2, thresholds2 = precision_recall_curve(y_true, y_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P-R 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出 P-R 曲线\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "# colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "\n",
    "y_pred= clf.predict(testDataArr)\n",
    "\n",
    "y_scores= clf.predict_proba(testDataArr)\n",
    "\n",
    "y_true=testLabelArr\n",
    "\n",
    "\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "\n",
    "y_scores2= clf2.predict_proba(testDataArr)[:,1]  # 第 1 列 , 表示为 正例的概率\n",
    "\n",
    "precision2, recall2, thresholds2 = precision_recall_curve(y_true, y_scores2)\n",
    "\n",
    "\n",
    "\n",
    "# disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "# disp.plot() \n",
    "\n",
    "\n",
    "plt.plot(recall, precision, label=\"GDBT_2Classifier(xrh)\",color='navy' ) # \n",
    "\n",
    "plt.plot(recall2, precision2, label=\"GradientBoostingClassifier(sklearn)\",color='turquoise' )\n",
    "\n",
    "\n",
    "plt.title(' Precision-Recall curve ')\n",
    "\n",
    "# plt.ylim([0.0, 1.05]) # Y 轴的取值范围\n",
    "# plt.xlim([0.0, 1.0]) # X 轴的取值范围\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "\n",
    "plt.legend( loc=(0, -.38), prop=dict(size=14) ) # 图例\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P-R 关系图的 左右端点的讨论**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos= len(y_true[y_true==1]) / len(y_true)\n",
    "ratio_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 左端点\n",
    "recall2[-1], precision2[-1],thresholds2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 右端点\n",
    "recall2[0], precision2[0],thresholds2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_true, y_scores2)\n",
    "\n",
    "plt.plot( [0, 1], [0, 1], color='navy', linestyle='--' )\n",
    "\n",
    "\n",
    "plt.plot( fpr, tpr, label=\"GDBT_2Classifier(xrh)\",color='darkorange' ) # \n",
    "\n",
    "plt.plot( fpr2, tpr2, label=\"GradientBoostingClassifier(sklearn)\",color='turquoise' )\n",
    "\n",
    "# plt.xlim( [0.0, 1.0] )\n",
    "# plt.ylim( [0.0, 1.05] )\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "plt.legend( loc=(0, -.38), prop=dict(size=14) ) # 图例\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC 曲线的 左右端点的讨论**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 右端点\n",
    "fpr[-1], tpr[-1],thresholds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 左端点\n",
    "fpr[0], fpr[0],thresholds2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train=[  [6],\n",
    "           [12],\n",
    "           [14],\n",
    "           [18],\n",
    "           [20],\n",
    "           [65],\n",
    "           [31],\n",
    "           [40],\n",
    "           [1],\n",
    "           [2],\n",
    "           [100],\n",
    "           [101],\n",
    "           [65],\n",
    "           [54],\n",
    "           ]\n",
    "\n",
    "y_train= np.array([[0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [2], [2], [2], [2]]).ravel()\n",
    "\n",
    "X=X_train\n",
    "\n",
    "y=y_train\n",
    "\n",
    "max_iter=5\n",
    "\n",
    "N = np.shape(X)[0]  # 样本的个数\n",
    "\n",
    "\n",
    "\n",
    "K = len({ele for ele in y})  # y 中有多少种不同的标签,  K分类\n",
    "\n",
    "print('according to the training dataset : K={} classification task'.format(K))\n",
    "\n",
    "F = np.zeros( (K , N),dtype=float) # shape: (K,N)\n",
    "\n",
    "for k in range(K): # 遍历 所有的 类别\n",
    "\n",
    "    F[k,:] = len(y[y == k]) / len(y)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    softmax处理，将结果转化为概率\n",
    "\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.e ** X / ((np.e ** X).sum(axis=0))  # softmax处理，将结果转化为概率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.special import expit, logsumexp\n",
    "\n",
    "\n",
    "raw_predictions=F\n",
    "\n",
    "np.nan_to_num( np.exp(raw_predictions - logsumexp(raw_predictions, axis=0)) )\n",
    "\n",
    "\n",
    "softmax( F )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=1\n",
    "\n",
    "p = softmax( F )\n",
    "\n",
    "\n",
    "\n",
    "y_one_hot = ( y == np.array(range(K)).reshape(-1, 1) ).astype(\n",
    "    np.int8)  # 将 预测向量 扩展为 one-hot , shape: (K,N)\n",
    "\n",
    "p_k=p[0]\n",
    "y_one_hot_k = y_one_hot[0]\n",
    "\n",
    "r=y_one_hot_k - p_k\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "F_0 = np.array( [1,2,3],dtype=float)  # shape : (K,)\n",
    "    \n",
    "np.transpose([F_0] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(3085)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  GBDT_MultiClassifier 实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('E:\\\\python package\\\\python-project\\\\统计学习方法\\\\Statistical-Learning-Method_Code-master\\\\DecisionTree')\n",
    "\n",
    "from CartTree_regression_xrh import *\n",
    "\n",
    "from scipy.special import expit, logsumexp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDT_MultiClassifier:\n",
    "    \"\"\"\n",
    "\n",
    "    适用于 二分类 问题 的 梯度提升树\n",
    "\n",
    "    基分类器为 CART 回归树\n",
    "\n",
    "    Author: xrh\n",
    "    Date: 2021-04-18\n",
    "\n",
    "    ref: https://zhuanlan.zhihu.com/p/91652813\n",
    "\n",
    "    test1: 多分类任务\n",
    "\n",
    "    数据集：Mnist\n",
    "    参数: error_rate_threshold=0.01, max_iter=20, max_depth=3 , learning_rate=0.5\n",
    "    训练集数量：60000\n",
    "    测试集数量：10000\n",
    "    正确率： 0.915\n",
    "    模型训练时长：1542s\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, error_rate_threshold=0.05, max_iter=10, max_depth=1):\n",
    "        \"\"\"\n",
    "\n",
    "        :param error_rate_threshold: 训练中止条件, 若当前得到的基分类器的组合 的错误率 小于阈值, 则停止训练\n",
    "        :param max_iter: 最大迭代次数\n",
    "        :param max_depth: CART 回归树 的最大深度\n",
    "        \"\"\"\n",
    "\n",
    "        # 训练中止条件 error_rate  < self.error_rate_threshold ( 若当前得到的基分类器的组合 的错误率 小于阈值, 则停止训练)\n",
    "        self.error_rate_threshold = error_rate_threshold\n",
    "\n",
    "        # 最大迭代次数\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # CART 回归树 的最大深度\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.G = []  # 弱分类器 集合\n",
    "\n",
    "    def sigmoid( self , X ):\n",
    "        \"\"\"\n",
    "        sigmoid 激活函数\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    # def softmax_deprecated(self,X):\n",
    "    #     \"\"\"\n",
    "    #     softmax处理，将结果转化为概率\n",
    "    #\n",
    "    #     :param X:\n",
    "    #     :return:\n",
    "    #     \"\"\"\n",
    "    #     #TODO: 导致 上溢出 和 下溢出 问题\n",
    "    #\n",
    "    #     return  np.exp(X) / np.sum( np.exp(X) , axis=0 )  # softmax处理，将结果转化为概率\n",
    "\n",
    "    def softmax_deprecated(self,X):\n",
    "        \"\"\"\n",
    "        softmax处理，将结果转化为概率\n",
    "\n",
    "        解决了 softmax的 上溢出 和 下溢出的问题\n",
    "\n",
    "        ref: https://www.cnblogs.com/guoyaohua/p/8900683.html\n",
    "\n",
    "        :param X: shape (K,N)\n",
    "        :return: shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        X_max= np.max( X, axis=0)\n",
    "        X= X-X_max\n",
    "\n",
    "        return  np.exp(X) / np.sum( np.exp(X) , axis=0 )  # softmax处理，将结果转化为概率\n",
    "\n",
    "    def softmax(self,X):\n",
    "        \"\"\"\n",
    "        softmax处理，将结果转化为概率\n",
    "\n",
    "        解决了 softmax的 溢出问题\n",
    "\n",
    "        np.nan_to_num : 使用0代替数组x中的nan元素，使用有限的数字代替inf元素\n",
    "\n",
    "        ref: sklearn 源码\n",
    "            MultinomialDeviance -> def negative_gradient\n",
    "\n",
    "        :param X: shape (K,N)\n",
    "        :return: shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        return  np.nan_to_num( np.exp(X - logsumexp(X, axis=0)) )  # softmax处理，将结果转化为概率\n",
    "\n",
    "    def fit(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "\n",
    "        用 训练数据 拟合模型\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :param y: 标签数据 , shape=(N_sample,)\n",
    "        :param learning_rate: 学习率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        N = np.shape(X)[0]  # 样本的个数\n",
    "\n",
    "        self.K = len({ele for ele in y})  # y 中有多少种不同的标签,  K分类\n",
    "\n",
    "        print('according to the training dataset : K={} classification task'.format(self.K))\n",
    "\n",
    "        F_0 = np.zeros( (self.K ),dtype=float)  # shape : (K,)\n",
    "\n",
    "        for k in range(self.K): # 遍历 所有的 类别\n",
    "\n",
    "            F_0[k] = len(y[y == k]) / len(y)\n",
    "\n",
    "        self.G.append(F_0)\n",
    "\n",
    "        F = np.transpose([F_0] * N) # 对 F_0 进行复制,  shape : (K, N)\n",
    "\n",
    "        feature_value_set = RegresionTree.get_feature_value_set(X)  # 可供选择的特征集合 , 包括 (特征, 切分值)\n",
    "\n",
    "        y_one_hot = (y == np.array(range(self.K)).reshape(-1, 1)).astype(\n",
    "            np.int8)  # 将 预测向量 扩展为 one-hot , shape: (K,N)\n",
    "\n",
    "        for m in range(1,self.max_iter):  # 进行 第 m 轮迭代\n",
    "\n",
    "            p = self.softmax( F ) #  shape: (K,N)\n",
    "\n",
    "            DT_list=[]\n",
    "\n",
    "            for k in range(self.K): #  依次训练 K 个 二分类器\n",
    "\n",
    "                print( '======= train No.{} 2Classifier ======='.format(k) )\n",
    "\n",
    "                r = y_one_hot[k] - p[k]   # 残差 shape:(N,)\n",
    "\n",
    "                # 训练 用于 2分类的 回归树\n",
    "                DT = RegresionTree_GBDT(min_square_loss=0.1, max_depth=self.max_depth,print_log=False)\n",
    "\n",
    "                DT.fit(X, r, y_one_hot[k], feature_value_set=feature_value_set)\n",
    "\n",
    "                y_predict = (self.K / (self.K-1)) * ( DT.predict(X) ) #  shape:(N,)\n",
    "\n",
    "                DT_list.append(DT)\n",
    "\n",
    "                F[k] += learning_rate * y_predict  # F[k]  shape:(N,)\n",
    "\n",
    "                # print('======= end =======')\n",
    "\n",
    "\n",
    "            self.G.append( (learning_rate, DT_list) )  # 存储 基分类器\n",
    "\n",
    "            # 计算 当前 所有弱分类器加权 得到的 最终分类器 的 分类错误率\n",
    "\n",
    "            G = self.softmax( F )\n",
    "\n",
    "            G_label = np.argmax( G, axis=0 )  # 取 概率最大的 作为 预测的标签\n",
    "\n",
    "            err_arr = np.ones( N, dtype=int )\n",
    "            err_arr[G_label == y] = 0\n",
    "            err_rate = np.mean(err_arr)  # 计算训练误差\n",
    "\n",
    "            print('round:{}, err_rate:{}'.format(m, err_rate))\n",
    "            print('======================')\n",
    "\n",
    "            if err_rate < self.error_rate_threshold:  # 错误率 已经小于 阈值, 则停止训练\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        对 测试 数据进行预测, 返回预测的标签\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N = np.shape(X)[0]  # 样本的个数\n",
    "\n",
    "        F_0 = self.G[0]  # G中 第一个 存储的是 初始化情况\n",
    "\n",
    "        F = np.transpose([F_0] * N)  # shape : (K, N)\n",
    "\n",
    "        for alpha, DT_list in self.G[1:]:\n",
    "\n",
    "            for k in range(self.K):\n",
    "\n",
    "                DT = DT_list[k]\n",
    "\n",
    "                y_predict = (self.K / (self.K - 1)) * (DT.predict(X))  # shape:(N,)\n",
    "\n",
    "                F[k] += alpha * y_predict  # F[k]  shape:(N,)\n",
    "\n",
    "        G = self.softmax(F)\n",
    "\n",
    "        G_label = np.argmax(G, axis=0)\n",
    "\n",
    "        return G_label\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        对 测试 数据进行预测, 返回预测的 概率值\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        F = self.G[0]  # 第一个 存储的是 初始化情况\n",
    "\n",
    "        for alpha, DT_list in self.G[1:]:\n",
    "\n",
    "            for k in range(self.K):\n",
    "                DT = DT_list[k]\n",
    "\n",
    "                y_predict = (self.K / (self.K - 1)) * (DT.predict(X))  # shape:(N,)\n",
    "\n",
    "                DT_list.append(DT)\n",
    "\n",
    "                F[k] += alpha * y_predict  # F[k]  shape:(N,)\n",
    "\n",
    "        G = self.softmax(F)\n",
    "\n",
    "        return G\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        使用 测试数据集 对模型进行评价, 返回正确率\n",
    "\n",
    "        :param X: 特征数据 , shape=(N_sample, N_feature)\n",
    "        :param y: 标签数据 , shape=(N_sample,)\n",
    "        :return:  正确率 accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        N = np.shape(X)[0]  # 样本的个数\n",
    "\n",
    "        G = self.predict(X)\n",
    "\n",
    "        err_arr = np.ones(N, dtype=int)\n",
    "        err_arr[G == y] = 0\n",
    "        err_rate = np.mean(err_arr)\n",
    "\n",
    "        accuracy = 1 - err_rate\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Test:   \n",
    "    \n",
    "    def loadData(self, fileName, n=1000):\n",
    "        '''\n",
    "        加载文件\n",
    "\n",
    "        :param fileName:要加载的文件路径\n",
    "        :param n: 返回的数据集的规模\n",
    "        :return: 数据集和标签集\n",
    "        '''\n",
    "        # 存放数据及标记\n",
    "        dataArr = []\n",
    "        labelArr = []\n",
    "        # 读取文件\n",
    "        fr = open(fileName)\n",
    "\n",
    "        cnt = 0  # 计数器\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for line in fr.readlines():\n",
    "\n",
    "\n",
    "            if cnt == n:\n",
    "                break\n",
    "\n",
    "            # 获取当前行，并按“，”切割成字段放入列表中\n",
    "            # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "            # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "            curLine = line.strip().split(',')\n",
    "            # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "            # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "            # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "            dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "            # 将标记信息放入标记集中\n",
    "            labelArr.append(int(curLine[0]))\n",
    "            cnt += 1\n",
    "\n",
    "        fr.close()\n",
    "\n",
    "        # 返回数据集和标记\n",
    "        return dataArr, labelArr\n",
    "\n",
    "    def test_Mnist_dataset(self, n_train, n_test):\n",
    "        \"\"\"\n",
    "         Mnist (手写数字) 数据集\n",
    "\n",
    "        测试 AdaBoost  的 多分类\n",
    "\n",
    "        :param n_train: 使用训练数据集的规模\n",
    "        :param n_test: 使用测试数据集的规模\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取训练集\n",
    "        trainDataList, trainLabelList = self.loadData('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "        print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "        trainDataArr = np.array(trainDataList)\n",
    "        trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "        # 开始时间\n",
    "        print('start training model....')\n",
    "        start = time.time()\n",
    "\n",
    "        \"\"\"\n",
    "        调参：\n",
    "        loss：损失函数。有deviance和exponential两种。deviance是采用对数似然，exponential是指数损失，后者相当于AdaBoost。\n",
    "        n_estimators:最大弱学习器个数，默认是100，调参时要注意过拟合或欠拟合，一般和learning_rate一起考虑。\n",
    "        criterion: 切分叶子节点时, 选择切分特征考虑的误差函数, 默认是 “ friedman_mse”（ Friedman 均方误差），“ mse”（均方误差）和“ mae”（均绝对误差）\n",
    "        learning_rate:步长，即每个弱学习器的权重缩减系数，默认为0.1，取值范围0-1，当取值为1时，相当于权重不缩减。较小的learning_rate相当于更多的迭代次数。\n",
    "        subsample:子采样，默认为1，取值范围(0,1]，当取值为1时，相当于没有采样。小于1时，即进行采样，按比例采样得到的样本去构建弱学习器。这样做可以防止过拟合，但是值不能太低，会造成高方差。\n",
    "        init：初始化弱学习器。不使用的话就是第一轮迭代构建的弱学习器.如果没有先验的话就可以不用管\n",
    "        由于GBDT使用CART回归决策树。以下参数用于调优弱学习器，主要都是为了防止过拟合\n",
    "        max_feature：树分裂时考虑的最大特征数，默认为None，也就是考虑所有特征。可以取值有：log2,auto,sqrt\n",
    "        max_depth：CART最大深度，默认为None\n",
    "        min_sample_split：划分节点时需要保留的样本数。当某节点的样本数小于某个值时，就当做叶子节点，不允许再分裂。默认是2\n",
    "        min_sample_leaf：叶子节点最少样本数。如果某个叶子节点数量少于某个值，会同它的兄弟节点一起被剪枝。默认是1\n",
    "        min_weight_fraction_leaf：叶子节点最小的样本权重和。如果小于某个值，会同它的兄弟节点一起被剪枝。一般用于权重变化的样本。默认是0\n",
    "        min_leaf_nodes：最大叶子节点数\n",
    "        \n",
    "        ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "        \n",
    "        测试1: \n",
    "        max_depth=3, n_estimators=30, learning_rate=0.8, \n",
    "        n_train=60000\n",
    "        n_test=10000\n",
    "        训练时间 : 795.5719292163849\n",
    "        准确率: 0.8883 \n",
    "        \n",
    "        测试2:\n",
    "        max_depth=3, n_estimators=20, learning_rate=0.5, \n",
    "        n_train=60000\n",
    "        n_test=10000\n",
    "        训练时间 : 589 s\n",
    "        准确率: 0.9197 \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        clf = GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=20, learning_rate=0.5,\n",
    "                                         max_depth=3)\n",
    "\n",
    "        clf.fit(trainDataArr, trainLabelArr)\n",
    "\n",
    "\n",
    "\n",
    "        # clf = GBDT_MultiClassifier( error_rate_threshold=0.01, max_iter=20, max_depth=3 )\n",
    "        # clf.fit( trainDataArr, trainLabelArr,learning_rate= 0.5 ) #\n",
    "\n",
    "        # 结束时间\n",
    "        end = time.time()\n",
    "        print('training cost time :', end - start)\n",
    "\n",
    "        # 获取测试集\n",
    "        testDataList, testLabelList = self.loadData('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "        print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "        testDataArr = np.array(testDataList)\n",
    "        testLabelArr = np.array(testLabelList)\n",
    "\n",
    "        print('test dataset accuracy: {} '.format(clf.score(testDataArr, testLabelArr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Test()\n",
    "\n",
    "test.test_Mnist_dataset(6000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData( fileName, n=1000):\n",
    "        '''\n",
    "        加载文件\n",
    "\n",
    "        :param fileName:要加载的文件路径\n",
    "        :param n: 返回的数据集的规模\n",
    "        :return: 数据集和标签集\n",
    "        '''\n",
    "        # 存放数据及标记\n",
    "        dataArr = []\n",
    "        labelArr = []\n",
    "        # 读取文件\n",
    "        fr = open(fileName)\n",
    "\n",
    "        cnt = 0  # 计数器\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for line in fr.readlines():\n",
    "\n",
    "\n",
    "            if cnt == n:\n",
    "                break\n",
    "\n",
    "            # 获取当前行，并按“，”切割成字段放入列表中\n",
    "            # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "            # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "            curLine = line.strip().split(',')\n",
    "            # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "            # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "            # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "            dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "            # 将标记信息放入标记集中\n",
    "            labelArr.append(int(curLine[0]))\n",
    "            cnt += 1\n",
    "\n",
    "        fr.close()\n",
    "\n",
    "        # 返回数据集和标记\n",
    "        return dataArr, labelArr\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_train=60000\n",
    "n_test=10000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "# 开始时间\n",
    "print('start training model....')\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# clf = GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=40, learning_rate=0.5,\n",
    "#                                  max_depth=3)\n",
    "\n",
    "# clf.fit(trainDataArr, trainLabelArr)\n",
    "\n",
    "\n",
    "clf = GBDT_MultiClassifier( error_rate_threshold=0.01, max_iter=70, max_depth=4 )\n",
    "clf.fit( trainDataArr, trainLabelArr,learning_rate= 0.5 ) #\n",
    "\n",
    "# 结束时间\n",
    "end = time.time()\n",
    "print('training cost time :', end - start)\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n",
    "\n",
    "print('test dataset accuracy: {} '.format(clf.score(testDataArr, testLabelArr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sklearn 调参 \n",
    "\n",
    "loss：损失函数。有deviance和exponential两种。deviance是采用对数似然，exponential是指数损失，后者相当于AdaBoost。\n",
    "\n",
    "n_estimators:最大弱学习器个数，默认是100，调参时要注意过拟合或欠拟合，一般和learning_rate一起考虑。\n",
    "\n",
    "criterion: 切分叶子节点时, 选择切分特征考虑的误差函数, 默认是 “ friedman_mse”（ Friedman 均方误差），“ mse”（均方误差）和“ mae”（均绝对误差）\n",
    "\n",
    "learning_rate:步长，即每个弱学习器的权重缩减系数，默认为0.1，取值范围0-1，当取值为1时，相当于权重不缩减。较小的learning_rate相当于更多的迭代次数。\n",
    "\n",
    "subsample:子采样，默认为1，取值范围(0,1]，当取值为1时，相当于没有采样。小于1时，即进行采样，按比例采样得到的样本去构建弱学习器。这样做可以防止过拟合，但是值不能太低，会造成高方差。\n",
    "\n",
    "init：初始化弱学习器。不使用的话就是第一轮迭代构建的弱学习器.如果没有先验的话就可以不用管\n",
    "\n",
    "由于GBDT使用CART回归决策树。以下参数用于调优弱学习器，主要都是为了防止过拟合\n",
    "\n",
    "max_feature：树分裂时考虑的最大特征数，默认为None，也就是考虑所有特征。可以取值有：log2,auto,sqrt\n",
    "\n",
    "max_depth：CART最大深度，默认为None\n",
    "\n",
    "min_sample_split：划分节点时需要保留的样本数。当某节点的样本数小于某个值时，就当做叶子节点，不允许再分裂。默认是2\n",
    "\n",
    "min_sample_leaf：叶子节点最少样本数。如果某个叶子节点数量少于某个值，会同它的兄弟节点一起被剪枝。默认是1\n",
    "\n",
    "min_weight_fraction_leaf：叶子节点最小的样本权重和。如果小于某个值，会同它的兄弟节点一起被剪枝。一般用于权重变化的样本。默认是0\n",
    "\n",
    "min_leaf_nodes：最大叶子节点数\n",
    "\n",
    "\n",
    "ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "测试1: \n",
    "max_depth=3, n_estimators=30, learning_rate=0.8, \n",
    "n_train=60000\n",
    "n_test=10000\n",
    "训练时间 : 795.5719292163849\n",
    "准确率: 0.8883 \n",
    "\n",
    "测试2:\n",
    "max_depth=3, n_estimators=20, learning_rate=0.5, \n",
    "n_train=60000\n",
    "n_test=10000\n",
    "训练时间 : 589 s\n",
    "准确率: 0.9197 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LinearRegression as LinearR\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTS\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#来查看一下sklearn中所有的 模型评估指标\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=6000\n",
    "n_test=1000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv', n=n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "Xtrain,Ytrain = trainDataArr, trainLabelArr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator,title, X, y, \n",
    "                        ax=None, #选择子图\n",
    "                        ylim=None, #设置纵坐标的取值范围\n",
    "                        cv=None, #交叉验证\n",
    "                        n_jobs=None #设定索要使用的线程\n",
    "                       ):\n",
    "    \n",
    "    from sklearn.model_selection import learning_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y\n",
    "                                                            ,shuffle=True\n",
    "                                                            ,cv=cv\n",
    "                                                            ,random_state=420\n",
    "                                                            ,n_jobs=n_jobs)      \n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = plt.figure()\n",
    "    ax.set_title(title)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    ax.set_xlabel(\"Training examples\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid() #绘制网格，不是必须\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1), 'o-'\n",
    "            , color=\"r\",label=\"Training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1), 'o-'\n",
    "            , color=\"g\",label=\"Test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle = True, random_state=42) # k 折 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold ref:\n",
    "\n",
    "https://blog.csdn.net/kancy110/article/details/74910185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index , test_index in cv.split(Xtrain):\n",
    "    print('train_index:%s , test_index: %s ' %(train_index,test_index))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=20, learning_rate=0.5,\n",
    "                                 max_depth=3)\n",
    "                    ,\"GBDT\",Xtrain,Ytrain,ax=None,cv=cv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=20, learning_rate=0.5,\n",
    "                                 max_depth=3)\n",
    "\n",
    "CVS( clf, Xtrain,Ytrain ,cv=5,scoring='accuracy') #  cv : 交叉验证的 折数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbdt = GBDT_MultiClassifier( error_rate_threshold=0.01, max_iter=20, max_depth=3 )\n",
    "# CVS( gbdt, Xtrain,Ytrain ,cv=5,scoring='accuracy') #  cv : 交叉验证的 折数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数 n_estimators 调优\n",
    "#=====【TIME WARNING： seconds】=====#\n",
    "\n",
    "axisx = range(10,100,10)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    \n",
    "    reg =  GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=i, learning_rate=0.5,\n",
    "                                 max_depth=3)\n",
    "    rs.append( CVS(reg,Xtrain,Ytrain,cv=cv).mean() )\n",
    "    \n",
    "print( axisx[rs.index(max(rs))], max(rs) ) # n_estimators=80  accuracy=0.9189999999999999\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot( axisx,rs,c=\"red\",label=\"GBDT\" )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数 learning_rate 调优\n",
    "\n",
    "axisx = np.linspace(0.5,1,10)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    \n",
    "    reg =  GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=20, learning_rate=i,\n",
    "                                 max_depth=3)\n",
    "    \n",
    "    rs.append( CVS(reg,Xtrain,Ytrain,cv=5,n_jobs=-1).mean() ) #  n_jobs=-1 使用所有的CPU核, 并行度实际为5, 因为使用 5折交叉验证       ，\n",
    "    \n",
    "    \n",
    "print( axisx[rs.index(max(rs))], max(rs) ) \n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot( axisx,rs,c=\"red\",label=\"GBDT\" )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网格搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "https://www.cnblogs.com/wj-1314/p/10422159.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#首先我们先来定义一个评分函数，这个评分函数能够帮助我们直接打印Xtrain上的交叉验证结果\n",
    "def clfassess(clf,Xtrain,Ytrain,cv,scoring = [\"accuracy\"],show=True):\n",
    "    \n",
    "    score = []\n",
    "    for i in range(len(scoring)):\n",
    "        \n",
    "        c=CVS (clf,Xtrain,Ytrain,cv=5,scoring=scoring[i]).mean()\n",
    "        \n",
    "        if show:\n",
    "            print(\"{}:{:.2f}\".format(scoring[i] #模型评估指标的名字\n",
    "                                ,c))\n",
    "            \n",
    "        score.append((c).mean())\n",
    "        \n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def print_best_score(gsearch,param_test):\n",
    "     # 输出best score\n",
    "    print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    # 输出最佳的分类器到底使用了怎样的参数\n",
    "    best_parameters = gsearch.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_test.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = {\n",
    "    'n_estimators':  range(20,90,10),\n",
    "    'max_depth': range(1,5,1)\n",
    "}\n",
    "#网格搜索 是 两个 参数集合的全组合(笛卡尔积), 因此 集合中的元素个数 不宜过多\n",
    "\n",
    "estimator=GradientBoostingClassifier(loss='deviance',criterion='mse', n_estimators=20, learning_rate=0.5,\n",
    "                                 max_depth=3)\n",
    "\n",
    "gsearch = GridSearchCV( estimator , param_grid = param, scoring='accuracy', cv=5 , n_jobs=-1 )\n",
    "\n",
    "gsearch.fit( Xtrain,Ytrain )\n",
    "\n",
    "\n",
    "print_best_score(gsearch,param)\n",
    "\n",
    "# Best score: 0.924\n",
    "# Best parameters set:\n",
    "# \tmax_depth: 4\n",
    "# \tn_estimators: 70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "301.6px",
    "left": "242px",
    "top": "65.7337px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
