{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 陈天奇的  XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LinearRegression as LinearR\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTS\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from xgboost import XGBClassifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb实现法\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.1,random_state=420)\n",
    "\n",
    "#使用类DMatrix读取数据\n",
    "dtrain = xgb.DMatrix( Xtrain,Ytrain ) #特征矩阵和标签都进行一个传入\n",
    "dtest = xgb.DMatrix( Xtest,Ytest )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写明参数\n",
    "param = {\n",
    "          'objective':'reg:squarederror'\n",
    "         ,\"eta\":0.1}\n",
    "num_round = 250 #n_estimators\n",
    "\n",
    "#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "#接口predict\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "MSE(Ytest,preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData( fileName, n=1000):\n",
    "        '''\n",
    "        加载文件\n",
    "\n",
    "        :param fileName:要加载的文件路径\n",
    "        :param n: 返回的数据集的规模\n",
    "        :return: 数据集和标签集\n",
    "        '''\n",
    "        # 存放数据及标记\n",
    "        dataArr = []\n",
    "        labelArr = []\n",
    "        # 读取文件\n",
    "        fr = open(fileName)\n",
    "\n",
    "        cnt = 0  # 计数器\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for line in fr.readlines():\n",
    "\n",
    "\n",
    "            if cnt == n:\n",
    "                break\n",
    "\n",
    "            # 获取当前行，并按“，”切割成字段放入列表中\n",
    "            # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "            # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "            curLine = line.strip().split(',')\n",
    "            # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "            # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "            # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "            dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "            # 将标记信息放入标记集中\n",
    "            labelArr.append(int(curLine[0]))\n",
    "            cnt += 1\n",
    "\n",
    "        fr.close()\n",
    "\n",
    "        # 返回数据集和标记\n",
    "        return dataArr, labelArr\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn 的 xgboost 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=60000\n",
    "n_test=10000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "# 开始时间\n",
    "print('start training model....')\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "XGBClassifier 测试1:\n",
    "max_depth=3, n_estimators=20, learning_rate=0.5, \n",
    "n_train=60000\n",
    "n_test=10000\n",
    "训练时间 : 38 s\n",
    "准确率: 0.9155\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    max_depth=3, #\n",
    "    learning_rate=0.5, # 学习率 eta \n",
    "    n_estimators=20, # 使用多少个弱分类器\n",
    "    \n",
    "    eval_metric='mlogloss',\n",
    "    \n",
    "    num_class=10,\n",
    "   \n",
    "    gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=1, # 随机抽样的时候抽取的样本比例, 范围 (0,1]\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0, # L1 正则化的强度\n",
    "    reg_lambda=1, # L2 正则化的强度\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "clf.fit(trainDataArr, trainLabelArr)\n",
    "\n",
    "\n",
    "# 结束时间\n",
    "end = time.time()\n",
    "print('training cost time :', end - start)\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n",
    "\n",
    "print('test dataset accuracy: {} '.format(clf.score(testDataArr, testLabelArr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=6000\n",
    "n_test=1000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "Xtrain,Ytrain = trainDataArr, trainLabelArr \n",
    "\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n",
    "\n",
    "Xtest,Ytest = testDataArr, testLabelArr \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator,title, X, y, \n",
    "                        ax=None, #选择子图\n",
    "                        ylim=None, #设置纵坐标的取值范围\n",
    "                        cv=None, #交叉验证\n",
    "                        n_jobs=None #设定索要使用的线程\n",
    "                       ):\n",
    "    \n",
    "    from sklearn.model_selection import learning_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y\n",
    "                                                            ,shuffle=True\n",
    "                                                            ,cv=cv\n",
    "                                                            ,random_state=420\n",
    "                                                            ,n_jobs=n_jobs)      \n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = plt.figure()\n",
    "    ax.set_title(title)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    ax.set_xlabel(\"Training examples\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid() #绘制网格，不是必须\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1), 'o-'\n",
    "            , color=\"r\",label=\"Training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1), 'o-'\n",
    "            , color=\"g\",label=\"Test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(\n",
    "    max_depth=3, #\n",
    "    learning_rate=0.5, # 学习率 eta \n",
    "    n_estimators=20, # 使用多少个弱分类器\n",
    "    \n",
    "    eval_metric='mlogloss',\n",
    "    \n",
    "    num_class=10,\n",
    "   \n",
    "    gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=1, # 随机抽样的时候抽取的样本比例, 范围 (0,1]\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0, # L1 正则化的强度\n",
    "    reg_lambda=1, # L2 正则化的强度\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "plot_learning_curve(clf\n",
    "                    ,\"XGBoost\",Xtrain,Ytrain,ax=None,cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数调优 - 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 超参数 n_estimators 调优\n",
    "#=====【TIME WARNING： 6min 】=====#\n",
    "\n",
    "axisx = range(10,100,10)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        max_depth=3, #\n",
    "        learning_rate=0.5, # 学习率 eta \n",
    "        n_estimators=i, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        subsample=1, # 有放回的随机抽样 的时候抽取的样本比例, 范围 (0,1]\n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    rs.append( CVS( clf , Xtrain, Ytrain, cv=5 , n_jobs=-1).mean() ) #  n_jobs=-1 开启所有的 CPU 核\n",
    "    \n",
    "print( axisx[rs.index(max(rs))], max(rs) ) # n_estimators=90   accuracy= 0.932\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot( axisx,rs,c=\"red\",label=\"XGBoost\" )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 超参数 learning_rate 调优\n",
    "#=====【TIME WARNING：12min  】=====#\n",
    "\n",
    "\n",
    "axisx = np.linspace(0.1,1,10)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        max_depth=3, #\n",
    "        learning_rate=i, # 学习率 eta \n",
    "        n_estimators=90, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        subsample=1, # 有放回的随机抽样 的时候抽取的样本比例, 范围 (0,1]\n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    rs.append( CVS( clf,Xtrain,Ytrain,cv=5 ).mean() ) # \n",
    "    \n",
    "    \n",
    "print( \"best param:{} , score:{}\".format( axisx[rs.index(max(rs))], max(rs) )) # learning_rate=0.4 accuracy=0.9335000000000001\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot( axisx,rs,c=\"red\",label=\"XGBoost\" )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 超参数 subsample 调优\n",
    "\n",
    "#=====【TIME WARNING：9 min 】=====#\n",
    "\n",
    "\n",
    "\n",
    "#首先我们先来定义一个评分函数，这个评分函数能够帮助我们直接打印Xtrain上的交叉验证结果\n",
    "def clfassess(clf,Xtrain,Ytrain,scoring = [\"accuracy\"],show=True):\n",
    "    \n",
    "    score = []\n",
    "    for i in range(len(scoring)):\n",
    "        \n",
    "        c=CVS (clf,Xtrain,Ytrain,cv=5,scoring=scoring[i]).mean()\n",
    "        \n",
    "        if show:\n",
    "            print(\"{}:{:.2f}\".format(scoring[i] #模型评估指标的名字\n",
    "                                ,c))\n",
    "            \n",
    "        score.append((c).mean())\n",
    "        \n",
    "    return score\n",
    "\n",
    "axisx = np.linspace(0.5,1,5)\n",
    "rs = []\n",
    "te = []\n",
    "for i in axisx:\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        \n",
    "        max_depth=3, #\n",
    "        learning_rate=0.4, # 学习率 eta \n",
    "        n_estimators=90, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        \n",
    "        subsample=i, # 有放回的随机抽样 的时候抽取的样本比例, 范围 (0,1]\n",
    "        \n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    score = clfassess( clf, Xtrain, Ytrain, scoring = [\"accuracy\"], show=True)\n",
    "    \n",
    "    test = clf.fit( Xtrain,Ytrain ).score( Xtest, Ytest )\n",
    "    \n",
    "    rs.append(score[0])\n",
    "    te.append(test)\n",
    "    \n",
    "     \n",
    "print(\"best param:{} , score:{}\".format(axisx[rs.index(max(rs))],max(rs))) # subsample=0.625 accuracy=0.9338\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.plot(axisx,te,c=\"gray\",label=\"test\")\n",
    "plt.plot(axisx,rs,c=\"green\",label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看测试集 上的混淆矩阵\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n",
    "y_pred= clf.predict( testDataArr )\n",
    "y_true=testLabelArr\n",
    "\n",
    "\n",
    "confusion_matrix(y_true, y_pred) # \n",
    "\n",
    "sw = compute_sample_weight(class_weight='balanced',y=y_true)\n",
    "\n",
    "confusion_matrix(y_true, y_pred, sample_weight=sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数调优 - 网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#来查看一下sklearn中所有的 模型评估指标\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def print_best_score(gsearch,param_test):\n",
    "     # 输出best score\n",
    "    print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    # 输出最佳的分类器到底使用了怎样的参数\n",
    "    best_parameters = gsearch.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_test.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = {\n",
    "    'gamma':  [0,2,5],\n",
    "    'max_depth': range(1,5,1)\n",
    "}\n",
    "#网格搜索 是 两个 参数集合的全组合(笛卡尔积), 因此 集合中的元素个数 不宜过多\n",
    "\n",
    "estimator = XGBClassifier(\n",
    "        \n",
    "        max_depth=3, #\n",
    "        learning_rate=0.4, # 学习率 eta \n",
    "        n_estimators=90, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        \n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        \n",
    "        subsample=0.625, # 有放回的随机抽样, 抽取的样本比例, 范围 (0,1]\n",
    "        \n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "\n",
    "gsearch = GridSearchCV( estimator , param_grid = param, scoring='accuracy', cv=5 , n_jobs=-1 )\n",
    "\n",
    "gsearch.fit( Xtrain,Ytrain )\n",
    "\n",
    "\n",
    "print_best_score(gsearch,param)\n",
    "\n",
    "# Best score: 0.937\n",
    "# Best parameters set:\n",
    "# \tgamma: 0\n",
    "# \tmax_depth: 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData_2classification( fileName, n=1000):\n",
    "    '''\n",
    "    加载文件\n",
    "\n",
    "    将 数据集 的标签 转换为 二分类的标签\n",
    "\n",
    "    :param fileName:要加载的文件路径\n",
    "    :param n: 返回的数据集的规模\n",
    "    :return: 数据集和标签集\n",
    "    '''\n",
    "    # 存放数据及标记\n",
    "    dataArr = []\n",
    "    labelArr = []\n",
    "    # 读取文件\n",
    "    fr = open(fileName)\n",
    "\n",
    "    cnt = 0  # 计数器\n",
    "\n",
    "    # 遍历文件中的每一行\n",
    "    for line in fr.readlines():\n",
    "\n",
    "        if cnt == n:\n",
    "            break\n",
    "\n",
    "        # 获取当前行，并按“，”切割成字段放入列表中\n",
    "        # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "        # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "        curLine = line.strip().split(',')\n",
    "        # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "        # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "        # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "        dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "        # 将标记信息放入标记集中\n",
    "        # 转换成二分类任务\n",
    "        # 标签0设置为1，反之为0\n",
    "\n",
    "        # 显然这会导致 正负 样本的 分布不均衡, 1 的样本很少(10%), 而0 的很多\n",
    "        if int(curLine[0]) == 0:\n",
    "            labelArr.append(1)\n",
    "        else:\n",
    "            labelArr.append(0)\n",
    "\n",
    "        # if int(curLine[0]) <= 5:\n",
    "        #     labelArr.append(1)\n",
    "        # else:\n",
    "        #     labelArr.append(0)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    fr.close()\n",
    "\n",
    "    # 返回数据集和标记\n",
    "    return dataArr, labelArr\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=6000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList =loadData_2classification('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "\n",
    "n_test=1000\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData_2classification('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#使用类DMatrix读取数据\n",
    "dtrain = xgb.DMatrix( trainDataArr,trainLabelArr ) #特征矩阵和标签都进行一个传入\n",
    "dtest = xgb.DMatrix( testDataArr,testLabelArr )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pandas 查看样本\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(trainDataArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = XGBClassifier(\n",
    "        \n",
    "#         max_depth=3, #\n",
    "#         learning_rate=0.4, # 学习率 eta \n",
    "#         n_estimators=90, # 使用多少个弱分类器\n",
    "#         eval_metric='mlogloss',\n",
    "#         num_class=10,\n",
    "#         gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "#         min_child_weight=1,\n",
    "#         max_delta_step=0,\n",
    "#         subsample=0.625, # 有放回的随机抽样, 抽取的样本比例, 范围 (0,1]\n",
    "#         colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "#         reg_alpha=0, # L1 正则化的强度\n",
    "#         reg_lambda=1, # L2 正则化的强度\n",
    "#         use_label_encoder=False\n",
    "#     )\n",
    "\n",
    "\n",
    "# param= {'silent':True,'objective':'binary:logistic',\"eta\":0.4}\n",
    "\n",
    "param= {'eval_metric':'logloss',\"eta\":0.4,}\n",
    "\n",
    "\n",
    "num_round = 90 #n_estimators\n",
    "\n",
    "#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入\n",
    "bst = xgb.train( param, dtrain, num_round )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "y_pred =( bst.predict(dtest) > 0.5 ).astype(int) #  predict() 返回的是概率  \n",
    "\n",
    "y_true= testLabelArr\n",
    "\n",
    "# 1.正确率\n",
    "print('test dataset accuracy: {} '.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "print('====================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 样本不均衡问题\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( '0 负样本所占的比例: {} '.format( len(trainLabelArr[trainLabelArr==0])/len(trainLabelArr) ))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm, accuracy_score as accuracy ,recall_score as recall, roc_auc_score as auc\n",
    "\n",
    "\n",
    "#写明参数\n",
    "scale_pos_weight = [ 0.5 , 1 , 5 , 9 ,10]\n",
    "names = [\n",
    "    \n",
    "         \"negative vs positive: 0.5 \",\n",
    "         \"negative vs positive: 1\",\n",
    "         \"negative vs positive: 5\",\n",
    "         \"negative vs positive: 9\",\n",
    "         \"negative vs positive: 10\"\n",
    "        \n",
    "        ]\n",
    "\n",
    "\n",
    "[*zip(names,scale_pos_weight)]\n",
    "\n",
    "\n",
    "for name,i in zip(names,scale_pos_weight):\n",
    "    \n",
    "    param= { 'eval_metric':'logloss',\"eta\":0.4,\"scale_pos_weight\":i } # scale_pos_weight = 负样本 / 正样本\n",
    "    \n",
    "    num_round = 40\n",
    "    \n",
    "    clf = xgb.train(param, dtrain, num_round)\n",
    "    \n",
    "    preds = clf.predict(dtest)\n",
    "    \n",
    "    ypred = preds.copy()\n",
    "    ypred[preds > 0.5] = 1\n",
    "    ypred[ypred != 1] = 0\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    print(\"\\tAccuracy:{}\".format(accuracy(testLabelArr,ypred)))\n",
    "    print(\"\\tRecall:{}\".format(recall(testLabelArr,ypred)))\n",
    "    print(\"\\tAUC:{}\".format(auc(testLabelArr,preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定参数\n",
    "param1 = { 'eval_metric':'logloss',\"eta\":0.4,\"scale_pos_weight\":9 , \"gamma\":0 }\n",
    "param2 = { 'eval_metric':'logloss',\"eta\":0.4,\"scale_pos_weight\":9 , \"gamma\":5 }\n",
    "\n",
    "num_round = 40\n",
    "n_fold=5 # sklearn - KFold\n",
    "\n",
    "\n",
    "cvresult1 = xgb.cv(param1, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "\n",
    "cvresult2 = xgb.cv(param2, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(range(1,41),cvresult1.iloc[:,0],c=\"red\",label=\"train,gamma=0\")\n",
    "plt.plot(range(1,41),cvresult1.iloc[:,2],c=\"orange\",label=\"test,gamma=0\")\n",
    "plt.plot(range(1,41),cvresult2.iloc[:,0],c=\"green\",label=\"train,gamma=5\")\n",
    "plt.plot(range(1,41),cvresult2.iloc[:,2],c=\"blue\",label=\"test,gamma=5\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#看看类xgb.cv生成了什么结果？\n",
    "\n",
    "cvresult1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higgs 数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载数据集\n",
    "\n",
    "\n",
    "原始 Higgs 数据集\n",
    "ref:\n",
    "https://archive.ics.uci.edu/ml/datasets/HIGGS\n",
    "\n",
    "总记录数\n",
    "11000000\n",
    "\n",
    "\n",
    "论文中使用的大小\n",
    "Higgs 10M( million = 百万) dataset\n",
    "\n",
    "\n",
    "kaggle 竞赛数据集 \n",
    "ref:https://www.kaggle.com/c/higgs-boson/data\n",
    "\n",
    "总记录数\n",
    "550000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Higgs_dataset_path= '../dataset/higgs/kaggle'\n",
    "\n",
    "\n",
    "# 取前10 行 看看长啥样子\n",
    "data = pd.read_csv(Higgs_dataset_path+'/training.csv',skiprows=0,nrows =10 )\n",
    "\n",
    "data.shape\n",
    "data # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in training data, directly use numpy\n",
    "dtrain = np.loadtxt(Higgs_dataset_path+'/training.csv' , delimiter=',', skiprows=1, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "\n",
    "# converters 对最后一列进行转换\n",
    "\n",
    "print ('finish loading from csv ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label  = dtrain[:,32]\n",
    "data   = dtrain[:,1:31]\n",
    "\n",
    "test_size = 550000\n",
    "\n",
    "# rescale weight to make it same as test set\n",
    "weight = dtrain[:,31] * float(test_size) / len(label)\n",
    "\n",
    "sum_wpos = sum( weight[i] for i in range(len(label)) if label[i] == 1.0  )\n",
    "sum_wneg = sum( weight[i] for i in range(len(label)) if label[i] == 0.0  )\n",
    "\n",
    "# print weight statistics\n",
    "print ('weight statistics: wpos=%g, wneg=%g, ratio=%g' % ( sum_wpos, sum_wneg, sum_wneg/sum_wpos ))\n",
    "\n",
    "# construct xgboost.DMatrix from numpy array, treat -999.0 as missing value\n",
    "xgmat = xgb.DMatrix( data, label=label, missing = -999.0, weight=weight )\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use logistic regression loss, use raw prediction before logistic transformation\n",
    "# since we only need the rank\n",
    "param['objective'] = 'binary:logitraw'\n",
    "# scale weight of positive examples\n",
    "param['scale_pos_weight'] = sum_wneg/sum_wpos\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['eval_metric'] = 'auc'\n",
    "param['nthread'] = 16\n",
    "\n",
    "# you can directly throw param in, though we want to watch multiple metrics here\n",
    "plst = list(param.items())+[('eval_metric', 'ams@0.15')] # \n",
    "\n",
    "watchlist = [ (xgmat,'train') ]\n",
    "# boost 120 trees\n",
    "num_round = 120\n",
    "print ('loading data end, start to boost trees')\n",
    "bst = xgb.train( plst, xgmat, num_round, watchlist );\n",
    "# save out model\n",
    "bst.save_model('higgs.model')\n",
    "\n",
    "print ('finish training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**watchlist 使用** \n",
    "\n",
    "作用: 在训练的时候 查看模型的训练效果\n",
    "\n",
    "划分20%为验证集 (dval)，准备一个watchlist 给train和validation set ,这样我们能发现每一个round 的验证集预测结果，如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了( early stop )。\n",
    "\n",
    "训练效果的 评价指标 通过参数 'eval_metric' 控制\n",
    "\n",
    "eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "watchlist = [(dtrain,'train'),(dval,'val')]\n",
    "\n",
    "model = xgb.train(params,dtrain,num_boost_round=100,evals = watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model('higgs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取前10 行 看看长啥样子\n",
    "data = pd.read_csv(Higgs_dataset_path+'/test.csv',skiprows=0,nrows =10 )\n",
    "\n",
    "data.shape # 发现测试数据集 没有标签列, 模型预测完测试集后提交到 kaggle 平台验证\n",
    "data # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data in do training\n",
    "train = np.loadtxt(Higgs_dataset_path+'/training.csv', delimiter=',', skiprows=1, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "\n",
    "label  = train[:,32]\n",
    "data   = train[:,1:31]\n",
    "weight = train[:,31]\n",
    "dtrain = xgb.DMatrix( data, label=label, missing = -999.0, weight=weight )\n",
    "param = {'max_depth':6, 'eta':0.1, 'objective':'binary:logitraw', 'nthread':4}\n",
    "num_round = 120\n",
    "\n",
    "print ('running cross validation, with preprocessing function')\n",
    "# define the preprocessing function\n",
    "# used to return the preprocessed training, test data, and parameter\n",
    "# we can use this to do weight rescale, etc.\n",
    "# as a example, we try to set scale_pos_weight\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label==1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    wtrain *= sum_weight / sum(wtrain)\n",
    "    wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return (dtrain, dtest, param)\n",
    "\n",
    "# do cross validation, for each fold\n",
    "# the dtrain, dtest, param will be passed into fpreproc\n",
    "# then the return value of fpreproc will be used to generate\n",
    "# results of that fold\n",
    "\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,metrics={'ams@0.15', 'auc'}, seed = 0, fpreproc = fpreproc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 近似算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定参数\n",
    "param1 = { 'objective':'binary:logistic',\"eta\":0.1,\"max_depth\":6 , \"nthread\":16}\n",
    "param2 = { 'objective':'binary:logistic',\"eta\":0.1,\"max_depth\":6 , \"nthread\":16, \"tree_method\": 'approx', \"sketch_eps\":0.3}\n",
    "\n",
    "num_round = 90\n",
    "n_fold=5 # sklearn - KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cvresult1 = xgb.cv(param1, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "# 2min 18s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cvresult2 = xgb.cv(param2, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "# 1min 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(range(1,91),cvresult1.iloc[:,2],c=\"orange\",label=\"test,exact greedy\")\n",
    "plt.plot(range(1,91),cvresult2.iloc[:,2],c=\"blue\",label=\"test,global eps=0.3\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集的技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score , train_test_split \n",
    "\n",
    "\n",
    "data = np.loadtxt(Higgs_dataset_path+'/training.csv' , delimiter=',', skiprows=1,max_rows=10000, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "\n",
    "# max_rows 设置读取的行数\n",
    "# converters 对最后一列进行转换\n",
    "\n",
    "X  = data[:,1:31]\n",
    "y  = data[:,32]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**划分数据集**\n",
    "\n",
    "划分数据集为 训练集 验证集 和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "dtrain = xgb.DMatrix( X_train,label=y_train)\n",
    "dval = xgb.DMatrix( X_val,label=y_val)\n",
    "dtest = xgb.DMatrix( X_test,label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = { 'objective':'binary:logistic',\"eta\":0.1,\"max_depth\":3 , \"nthread\":16}\n",
    "\n",
    "watchlist = [(dtrain,'train'),(dval,'val')]\n",
    "\n",
    "# watchlist = [(dtrain,'train')]\n",
    "\n",
    "param1['eval_metric'] = 'auc'\n",
    "\n",
    "num_round = 120\n",
    "\n",
    "bst = xgb.train( param1 ,dtrain  ,num_round, evals =watchlist )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=bst.predict(dtest)\n",
    "\n",
    "ypred = preds.copy()\n",
    "ypred[preds > 0.5] = 1\n",
    "ypred[ypred != 1] = 0\n",
    "\n",
    "print(\"\\tAccuracy:{}\".format(accuracy(y_test,ypred)))\n",
    "print(\"\\tAUC:{}\".format(auc(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**划分数据集**\n",
    "\n",
    "分层抽样\n",
    "\n",
    "ref:https://blog.csdn.net/haoji007/article/details/106165488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(y[y ==1]) / len(y) # 原数据集的 正样本比例\n",
    "\n",
    "len(y_train[y_train ==1]) / len(y_train)# 训练数据集的 正样本比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "print(split )       \n",
    " \n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[y ==1]) / len(y) # 原数据集的 正样本比例\n",
    "\n",
    "len(y_train[y_train ==1]) / len(y_train)# 训练数据集的 正样本比例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏感知\n",
    "\n",
    "详见 Kaggle_Allstate_Claim_Prediction_Challenge.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MQ2008 数据集 描述\n",
    "\n",
    "每一行是一个查询文档对。第一列是 这个文档对的相关性的标签，第二列是查询id，下面的列是特性，行结尾是关于 文档对的注释，包括文档id。相关性标签越大，查询文档对越相关。查询文档对由46维的特征向量表示。以下是MQ2007数据集中的几个示例行:\n",
    "\n",
    "=================================\n",
    "\n",
    "2 qid:10032 1:0.056537 2:0.000000 3:0.666667 4:1.000000 5:0.067138 … 45:0.000000 46:0.076923 #docid = GX029-35-5894638 inc = 0.0119881192468859 prob = 0.139842\n",
    "\n",
    "0 qid:10032 1:0.279152 2:0.000000 3:0.000000 4:0.000000 5:0.279152 … 45:0.250000 46:1.000000 #docid = GX030-77-6315042 inc = 1 prob = 0.341364\n",
    "\n",
    "0 qid:10032 1:0.130742 2:0.000000 3:0.333333 4:0.000000 5:0.134276 … 45:0.750000 46:1.000000 #docid = GX140-98-13566007 inc = 1 prob = 0.0701303\n",
    "\n",
    "1 qid:10032 1:0.593640 2:1.000000 3:0.000000 4:0.000000 5:0.600707 … 45:0.500000 46:0.000000 #docid = GX256-43-0740276 inc = 0.0136292023050293 prob = 0.400738\n",
    "\n",
    "=================================\n",
    "\n",
    "\n",
    "ref:\n",
    "https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换\n",
    "\n",
    "import sys\n",
    "\n",
    "def save_data(group_data,output_feature,output_group):\n",
    "    if len(group_data) == 0:\n",
    "        return\n",
    "\n",
    "    output_group.write(str(len(group_data))+\"\\n\")\n",
    "    for data in group_data:\n",
    "\n",
    "        # only include nonzero features\n",
    "        feats = [ p for p in data[2:] if float(p.split(':')[1]) != 0.0 ]\n",
    "        output_feature.write(data[0] + \" \" + \" \".join(feats) + \"\\n\")\n",
    "\n",
    "\n",
    "# 传入参数:\n",
    "# ../../../dataset/MQ2008/Fold1/train.txt\n",
    "# ../../../dataset/MQ2008/mq2008.train\n",
    "# ../../../dataset/MQ2008/mq2008.train.group\n",
    "\n",
    "\n",
    "\n",
    "fi = open('../dataset/MQ2008/Fold1/train.txt')\n",
    "\n",
    "output_feature = open('../dataset/MQ2008/mq2008.train',\"w\")\n",
    "output_group = open('../dataset/MQ2008/mq2008.train.group',\"w\")\n",
    "\n",
    "group_data = []\n",
    "group = \"\"\n",
    "for line in fi:\n",
    "    if not line:\n",
    "        break\n",
    "    if \"#\" in line:\n",
    "        line = line[:line.index(\"#\")]\n",
    "    splits = line.strip().split(\" \")\n",
    "\n",
    "    if splits[1] != group:\n",
    "        save_data(group_data,output_feature,output_group)\n",
    "        group_data = []\n",
    "\n",
    "    group = splits[1]\n",
    "    group_data.append(splits)\n",
    "\n",
    "save_data(group_data,output_feature,output_group)\n",
    "\n",
    "fi.close()\n",
    "output_feature.close()\n",
    "output_group.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对原始 数据集进行处理 生成 特征文件 mq2008.train  和 分组文件 train.group\n",
    "\n",
    "(1) 特征文件 中第一列为 文档对 query 相关度的打分, 第二列开始为 特征id: 特征值 ,特征值为0 的特征被排除, 以下是几个实例行：\n",
    "\n",
    "0 1:0.007477 3:1.000000 5:0.007470 11:0.471076 13:1.000000 15:0.477541 16:0.005120\n",
    "0 1:0.603738 3:1.000000 5:0.603175 13:0.122130 16:0.998377 17:0.375000 18:1.000000\n",
    "0 1:0.214953 5:0.213819 11:0.401330 15:0.402388 16:0.140868 17:1.000000 18:0.285714 19:0.333333 20:0.141484 \n",
    "0 3:1.000000 11:0.458053 13:0.495975 15:0.461687 18:0.571429 19:0.833333 21:0.273864 22:0.148498 29:0.387106 \n",
    "\n",
    "(2) 分组文件 中每一行 代表一个 组, 每一行的数字代表这一组拥有的 样本的个数; (只有同一个 group 中的样本才有排序的意义。对于IR任务来说，不同 query对应不同group。)\n",
    "\n",
    "以下是几个实例行：\n",
    "8  当前group 拥有8个样本\n",
    "8\n",
    "8\n",
    "8\n",
    "8\n",
    "16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-map:0.70906\n",
      "[1]\tvalidation-map:0.72783\n",
      "[2]\tvalidation-map:0.72909\n",
      "[3]\tvalidation-map:0.73380\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "dir='../dataset/MQ2008/'\n",
    "\n",
    "#  This script demonstrate how to do ranking with xgboost.train\n",
    "x_train, y_train = load_svmlight_file(dir+\"mq2008.train\") # load_svmlight_file 载入 libsvm 格式的数据, 并将其转换为  CSR matrix\n",
    "x_valid, y_valid = load_svmlight_file(dir+\"mq2008.vali\")\n",
    "x_test, y_test = load_svmlight_file(dir+\"mq2008.test\")\n",
    "\n",
    "# libsvm 使用的文件格式如下：\n",
    "#\n",
    "#  [label] [index1]:[value1] [index2]:[value2] …\n",
    "#\n",
    "# label  目标值，就是说class（属于哪一类），就是你要分类的种类，通常是一些整数。\n",
    "# index 是有顺序的索引，通常是连续的整数。就是指特征编号，必须按照升序排列\n",
    "# value 就是特征值，用来train的数据，通常是一堆实数组成。\n",
    "\n",
    "# from scipy.sparse import csr_matrix\n",
    "# X = csr_matrix([[0, 0, 1], [2, 3, 0]])\n",
    "# X\n",
    "# <2x3 sparse matrix of type '<type 'numpy.int64'>'\n",
    "#     with 3 stored elements in Compressed Sparse Row format>\n",
    "# X.toarray()\n",
    "# array([[0, 0, 1],\n",
    "#        [2, 3, 0]])\n",
    "# print(X) #  仅仅是 打印出来的样子 , 并不代表实际的存储格式\n",
    "#   (0, 2)    1\n",
    "#   (1, 0)    2\n",
    "#   (1, 1)    3\n",
    "\n",
    "# x_train 采用 稀疏编码 (CSR) 进行存储\n",
    "\n",
    "\n",
    "group_train = []\n",
    "with open( dir+\"mq2008.train.group\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        group_train.append(int(line.split(\"\\n\")[0]))\n",
    "\n",
    "group_valid = []\n",
    "with open( dir+\"mq2008.vali.group\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        group_valid.append(int(line.split(\"\\n\")[0]))\n",
    "\n",
    "group_test = []\n",
    "with open(dir+\"mq2008.test.group\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        group_test.append( int(line.split(\"\\n\")[0]) )\n",
    "\n",
    "train_dmatrix = DMatrix(x_train, y_train)\n",
    "valid_dmatrix = DMatrix(x_valid, y_valid)\n",
    "test_dmatrix = DMatrix(x_test)\n",
    "\n",
    "# DMatrix有set_group方法，调用设置 groupId。\n",
    "# (groupId 的概念在 rank 中广泛适用，只有同一个 group 中的样本才有排序的意义。对于IR任务来说，不同 query对应不同group。)\n",
    "# 注意set_group 方法传入的是每个 group 中元素的个数，\n",
    "\n",
    "\n",
    "train_dmatrix.set_group(group_train)\n",
    "valid_dmatrix.set_group(group_valid)\n",
    "\n",
    "params = {'objective': 'rank:ndcg', 'eta': 0.1, 'gamma': 1.0,\n",
    "          'min_child_weight': 0.1, 'max_depth': 6}\n",
    "\n",
    "xgb_model = xgb.train(params, train_dmatrix, num_boost_round=4,\n",
    "                      evals=[(valid_dmatrix, 'validation')])\n",
    "\n",
    "pred = xgb_model.predict(test_dmatrix) # 输出对 文档对的打分 , 对这些分值进行排序 即可得到最后的 doc list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78897315, 0.17356825, 0.78815585, ..., 0.3806271 , 0.42701086,\n",
       "       0.17356825], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我的 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.array(range(3)).reshape(-1, 1)\n",
    "f=np.array([6,5,4]).reshape(-1, 1)\n",
    "\n",
    "f\n",
    "idx\n",
    "\n",
    "f_idx=np.concatenate([f,idx],axis=1)\n",
    "\n",
    "f_idx\n",
    "\n",
    "# np.sort(f_idx,axis=0 )\n",
    "# np.sort(f_idx,axis=1 )\n",
    "\n",
    "f_idx = f_idx[f_idx[:,0].argsort()] # 按照第0列 对行排序\n",
    "f_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_idx[:1+1,:]\n",
    "f_idx[1+1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_idx[:2+1,:]\n",
    "f_idx[2+1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left=f_idx[:1+1,:]\n",
    "right=f_idx[1+1:,:]\n",
    "\n",
    "index_left=left[:,1]\n",
    "index_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_k=np.array([[10, 0],\n",
    "                  [11, 1],\n",
    "                  [12, 2]])\n",
    "\n",
    "# block_k[:,1]==[1,2]\n",
    "\n",
    "# block_k[:,1]==[0,1,2]\n",
    "\n",
    "# block_k[ block_k[:,1]==[0,1,2] , : ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition= np.array([ True if sample_id in set(index_left) else False for sample_id in block_k[:,1]  ] )\n",
    "\n",
    "condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_k[ condition , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_not= ~condition\n",
    "block_k[ condition_not , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3494850021680094"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ln'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f02ed25772bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[1;32m--> 215\u001b[1;33m                                      \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ln'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "(1-1/np.log2(10))*0.5\n",
    "\n",
    "np.ln(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=0\n",
    "\n",
    "~a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GradeStats:\n",
    "    \"\"\"\n",
    "    梯度 统计信息\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # 一阶梯度的和\n",
    "        self.sum_grad=0\n",
    "\n",
    "        # 二阶梯度的和\n",
    "        self.sum_hess=0\n",
    "\n",
    "# GradeStats_list = [GradeStats()]*10\n",
    "        \n",
    "GradeStats_list = [GradeStats() for i in range(10)]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    GradeStats_list[i].sum_grad=i\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(GradeStats_list[i].sum_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1799015944768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(GradeStats_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
