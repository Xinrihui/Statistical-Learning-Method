{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 陈天奇的  XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LinearRegression as LinearR\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTS\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from xgboost import XGBClassifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb实现法\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.1,random_state=420)\n",
    "\n",
    "#使用类DMatrix读取数据\n",
    "dtrain = xgb.DMatrix( Xtrain,Ytrain ) #特征矩阵和标签都进行一个传入\n",
    "dtest = xgb.DMatrix( Xtest,Ytest )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写明参数\n",
    "param = {\n",
    "          'objective':'reg:squarederror'\n",
    "         ,\"eta\":0.1}\n",
    "num_round = 250 #n_estimators\n",
    "\n",
    "#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "#接口predict\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "MSE(Ytest,preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn 的 xgboost 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData( fileName, n=1000):\n",
    "        '''\n",
    "        加载文件\n",
    "\n",
    "        :param fileName:要加载的文件路径\n",
    "        :param n: 返回的数据集的规模\n",
    "        :return: 数据集和标签集\n",
    "        '''\n",
    "        # 存放数据及标记\n",
    "        dataArr = []\n",
    "        labelArr = []\n",
    "        # 读取文件\n",
    "        fr = open(fileName)\n",
    "\n",
    "        cnt = 0  # 计数器\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for line in fr.readlines():\n",
    "\n",
    "\n",
    "            if cnt == n:\n",
    "                break\n",
    "\n",
    "            # 获取当前行，并按“，”切割成字段放入列表中\n",
    "            # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "            # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "            curLine = line.strip().split(',')\n",
    "            # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "            # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "            # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "            dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "            # 将标记信息放入标记集中\n",
    "            labelArr.append(int(curLine[0]))\n",
    "            cnt += 1\n",
    "\n",
    "        fr.close()\n",
    "\n",
    "        # 返回数据集和标记\n",
    "        return dataArr, labelArr\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data, row num:6000 , column num:784 \n",
      "start training model....\n",
      "training cost time : 4.076283693313599\n",
      "test data, row num:1000 , column num:784 \n",
      "test dataset accuracy: 0.865 \n",
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "n_train=6000\n",
    "n_test=1000\n",
    "\n",
    "Mnist_dir = '../dataset/Mnist'\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList = loadData(os.path.join(Mnist_dir, 'mnist_train.csv'), n=n_train)\n",
    "\n",
    "print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "\n",
    "# 开始时间\n",
    "print('start training model....')\n",
    "start = time.time()\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    max_depth=3, #\n",
    "    learning_rate=0.5, # 学习率 eta \n",
    "    n_estimators=20, # 使用多少个弱分类器\n",
    "    eval_metric='mlogloss',\n",
    "    num_class=10,\n",
    "    gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=1, # 随机抽样的时候抽取的样本比例, 范围 (0,1]\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0, # L1 正则化的强度\n",
    "    reg_lambda=1, # L2 正则化的强度\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "clf.fit(trainDataArr, trainLabelArr)\n",
    "\n",
    "# 结束时间\n",
    "end = time.time()\n",
    "print('training cost time :', end - start)\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData(os.path.join(Mnist_dir, 'mnist_test.csv'), n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n",
    "\n",
    "print('test dataset accuracy: {} '.format(clf.score(testDataArr, testLabelArr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier 测试1:\n",
    "max_depth=3, n_estimators=20, learning_rate=0.5, \n",
    "n_train=6000\n",
    "n_test=1000\n",
    "训练时间 : 4 s\n",
    "准确率: 0.865\n",
    "\n",
    "\n",
    "XGBClassifier 测试2:\n",
    "max_depth=3, n_estimators=20, learning_rate=0.5, \n",
    "n_train=60000\n",
    "n_test=10000\n",
    "训练时间 : 38 s\n",
    "准确率: 0.9155\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=6000\n",
    "n_test=1000\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv', n=n_train)\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "Xtrain,Ytrain = trainDataArr, trainLabelArr \n",
    "\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData('../Mnist/mnist_test.csv', n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n",
    "\n",
    "Xtest,Ytest = testDataArr, testLabelArr \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator,title, X, y, \n",
    "                        ax=None, #选择子图\n",
    "                        ylim=None, #设置纵坐标的取值范围\n",
    "                        cv=None, #交叉验证\n",
    "                        n_jobs=None #设定索要使用的线程\n",
    "                       ):\n",
    "    \n",
    "    from sklearn.model_selection import learning_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y\n",
    "                                                            ,shuffle=True\n",
    "                                                            ,cv=cv\n",
    "                                                            ,random_state=420\n",
    "                                                            ,n_jobs=n_jobs)      \n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = plt.figure()\n",
    "    ax.set_title(title)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    ax.set_xlabel(\"Training examples\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid() #绘制网格，不是必须\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1), 'o-'\n",
    "            , color=\"r\",label=\"Training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1), 'o-'\n",
    "            , color=\"g\",label=\"Test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(\n",
    "    max_depth=3, #\n",
    "    learning_rate=0.5, # 学习率 eta \n",
    "    n_estimators=20, # 使用多少个弱分类器\n",
    "    \n",
    "    eval_metric='mlogloss',\n",
    "    \n",
    "    num_class=10,\n",
    "   \n",
    "    gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=1, # 随机抽样的时候抽取的样本比例, 范围 (0,1]\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0, # L1 正则化的强度\n",
    "    reg_lambda=1, # L2 正则化的强度\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "plot_learning_curve(clf\n",
    "                    ,\"XGBoost\",Xtrain,Ytrain,ax=None,cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数调优 - 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 超参数 n_estimators 调优\n",
    "#=====【TIME WARNING： 6min 】=====#\n",
    "\n",
    "axisx = range(10,100,10)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        max_depth=3, #\n",
    "        learning_rate=0.5, # 学习率 eta \n",
    "        n_estimators=i, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        subsample=1, # 有放回的随机抽样 的时候抽取的样本比例, 范围 (0,1]\n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    rs.append( CVS( clf , Xtrain, Ytrain, cv=5 , n_jobs=-1).mean() ) #  n_jobs=-1 开启所有的 CPU 核\n",
    "    \n",
    "print( axisx[rs.index(max(rs))], max(rs) ) # n_estimators=90   accuracy= 0.932\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot( axisx,rs,c=\"red\",label=\"XGBoost\" )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 超参数 learning_rate 调优\n",
    "#=====【TIME WARNING：12min  】=====#\n",
    "\n",
    "\n",
    "axisx = np.linspace(0.1,1,10)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        max_depth=3, #\n",
    "        learning_rate=i, # 学习率 eta \n",
    "        n_estimators=90, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        subsample=1, # 有放回的随机抽样 的时候抽取的样本比例, 范围 (0,1]\n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    rs.append( CVS( clf,Xtrain,Ytrain,cv=5 ).mean() ) # \n",
    "    \n",
    "    \n",
    "print( \"best param:{} , score:{}\".format( axisx[rs.index(max(rs))], max(rs) )) # learning_rate=0.4 accuracy=0.9335000000000001\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot( axisx,rs,c=\"red\",label=\"XGBoost\" )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 超参数 subsample 调优\n",
    "\n",
    "#=====【TIME WARNING：9 min 】=====#\n",
    "\n",
    "\n",
    "\n",
    "#首先我们先来定义一个评分函数，这个评分函数能够帮助我们直接打印Xtrain上的交叉验证结果\n",
    "def clfassess(clf,Xtrain,Ytrain,scoring = [\"accuracy\"],show=True):\n",
    "    \n",
    "    score = []\n",
    "    for i in range(len(scoring)):\n",
    "        \n",
    "        c=CVS (clf,Xtrain,Ytrain,cv=5,scoring=scoring[i]).mean()\n",
    "        \n",
    "        if show:\n",
    "            print(\"{}:{:.2f}\".format(scoring[i] #模型评估指标的名字\n",
    "                                ,c))\n",
    "            \n",
    "        score.append((c).mean())\n",
    "        \n",
    "    return score\n",
    "\n",
    "axisx = np.linspace(0.5,1,5)\n",
    "rs = []\n",
    "te = []\n",
    "for i in axisx:\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        \n",
    "        max_depth=3, #\n",
    "        learning_rate=0.4, # 学习率 eta \n",
    "        n_estimators=90, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        \n",
    "        subsample=i, # 有放回的随机抽样 的时候抽取的样本比例, 范围 (0,1]\n",
    "        \n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    score = clfassess( clf, Xtrain, Ytrain, scoring = [\"accuracy\"], show=True)\n",
    "    \n",
    "    test = clf.fit( Xtrain,Ytrain ).score( Xtest, Ytest )\n",
    "    \n",
    "    rs.append(score[0])\n",
    "    te.append(test)\n",
    "    \n",
    "     \n",
    "print(\"best param:{} , score:{}\".format(axisx[rs.index(max(rs))],max(rs))) # subsample=0.625 accuracy=0.9338\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.plot(axisx,te,c=\"gray\",label=\"test\")\n",
    "plt.plot(axisx,rs,c=\"green\",label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看测试集 上的混淆矩阵\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n",
    "y_pred= clf.predict( testDataArr )\n",
    "y_true=testLabelArr\n",
    "\n",
    "\n",
    "confusion_matrix(y_true, y_pred) # \n",
    "\n",
    "sw = compute_sample_weight(class_weight='balanced',y=y_true)\n",
    "\n",
    "confusion_matrix(y_true, y_pred, sample_weight=sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数调优 - 网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#来查看一下sklearn中所有的 模型评估指标\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def print_best_score(gsearch,param_test):\n",
    "     # 输出best score\n",
    "    print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    # 输出最佳的分类器到底使用了怎样的参数\n",
    "    best_parameters = gsearch.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_test.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = {\n",
    "    'gamma':  [0,2,5],\n",
    "    'max_depth': range(1,5,1)\n",
    "}\n",
    "#网格搜索 是 两个 参数集合的全组合(笛卡尔积), 因此 集合中的元素个数 不宜过多\n",
    "\n",
    "estimator = XGBClassifier(\n",
    "        \n",
    "        max_depth=3, #\n",
    "        learning_rate=0.4, # 学习率 eta \n",
    "        n_estimators=90, # 使用多少个弱分类器\n",
    "\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        num_class=10,\n",
    "\n",
    "        gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "        \n",
    "        min_child_weight=1,\n",
    "        max_delta_step=0,\n",
    "        \n",
    "        subsample=0.625, # 有放回的随机抽样, 抽取的样本比例, 范围 (0,1]\n",
    "        \n",
    "        colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "        reg_alpha=0, # L1 正则化的强度\n",
    "        reg_lambda=1, # L2 正则化的强度\n",
    "        \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "\n",
    "gsearch = GridSearchCV( estimator , param_grid = param, scoring='accuracy', cv=5 , n_jobs=-1 )\n",
    "\n",
    "gsearch.fit( Xtrain,Ytrain )\n",
    "\n",
    "\n",
    "print_best_score(gsearch,param)\n",
    "\n",
    "# Best score: 0.937\n",
    "# Best parameters set:\n",
    "# \tgamma: 0\n",
    "# \tmax_depth: 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData_2classification( fileName, n=1000):\n",
    "    '''\n",
    "    加载文件\n",
    "\n",
    "    将 数据集 的标签 转换为 二分类的标签\n",
    "\n",
    "    :param fileName:要加载的文件路径\n",
    "    :param n: 返回的数据集的规模\n",
    "    :return: 数据集和标签集\n",
    "    '''\n",
    "    # 存放数据及标记\n",
    "    dataArr = []\n",
    "    labelArr = []\n",
    "    # 读取文件\n",
    "    fr = open(fileName)\n",
    "\n",
    "    cnt = 0  # 计数器\n",
    "\n",
    "    # 遍历文件中的每一行\n",
    "    for line in fr.readlines():\n",
    "\n",
    "        if cnt == n:\n",
    "            break\n",
    "\n",
    "        # 获取当前行，并按“，”切割成字段放入列表中\n",
    "        # strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "        # split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "        curLine = line.strip().split(',')\n",
    "        # 将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "        # 在放入的同时将原先字符串形式的数据转换为整型\n",
    "        # 此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "        dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "\n",
    "        # 将标记信息放入标记集中\n",
    "        # 转换成二分类任务\n",
    "        # 标签0设置为1，反之为0\n",
    "\n",
    "        # 显然这会导致 正负 样本的 分布不均衡, 1 的样本很少(10%), 而0 的很多\n",
    "        if int(curLine[0]) == 0:\n",
    "            labelArr.append(1)\n",
    "        else:\n",
    "            labelArr.append(0)\n",
    "\n",
    "        # if int(curLine[0]) <= 5:\n",
    "        #     labelArr.append(1)\n",
    "        # else:\n",
    "        #     labelArr.append(0)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    fr.close()\n",
    "\n",
    "    # 返回数据集和标记\n",
    "    return dataArr, labelArr\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data, row num:6000 , column num:784 \n",
      "test data, row num:1000 , column num:784 \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "n_train=6000\n",
    "\n",
    "Mnist_dir = '../dataset/Mnist'\n",
    "\n",
    "# 获取训练集\n",
    "trainDataList, trainLabelList =loadData_2classification(os.path.join(Mnist_dir, 'mnist_train.csv'), n=n_train)\n",
    "\n",
    "print('train data, row num:{} , column num:{} '.format(len(trainDataList), len(trainDataList[0])))\n",
    "\n",
    "trainDataArr = np.array(trainDataList)\n",
    "trainLabelArr = np.array(trainLabelList)\n",
    "\n",
    "\n",
    "n_test=1000\n",
    "\n",
    "# 获取测试集\n",
    "testDataList, testLabelList = loadData_2classification(os.path.join(Mnist_dir, 'mnist_test.csv'), n=n_test)\n",
    "\n",
    "print('test data, row num:{} , column num:{} '.format(len(testDataList), len(testDataList[0])))\n",
    "\n",
    "testDataArr = np.array(testDataList)\n",
    "testLabelArr = np.array(testLabelList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pandas 查看样本\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(trainDataArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset accuracy: 0.975 \n",
      "====================\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#使用类DMatrix读取数据\n",
    "dtrain = xgb.DMatrix( trainDataArr,trainLabelArr ) #特征矩阵和标签都进行一个传入\n",
    "dtest = xgb.DMatrix( testDataArr,testLabelArr )\n",
    "\n",
    "# estimator = XGBClassifier(\n",
    "        \n",
    "#         max_depth=3, #\n",
    "#         learning_rate=0.4, # 学习率 eta \n",
    "#         n_estimators=90, # 使用多少个弱分类器\n",
    "#         eval_metric='mlogloss',\n",
    "#         num_class=10,\n",
    "#         gamma=0, # 损失函数中 树的总叶子个数T 的系数, 可以控制模型的复杂度\n",
    "#         min_child_weight=1,\n",
    "#         max_delta_step=0,\n",
    "#         subsample=0.625, # 有放回的随机抽样, 抽取的样本比例, 范围 (0,1]\n",
    "#         colsample_bytree=1, # 构造 每棵树 随机抽样出的特征占总特征的比例\n",
    "#         reg_alpha=0, # L1 正则化的强度\n",
    "#         reg_lambda=1, # L2 正则化的强度\n",
    "#         use_label_encoder=False\n",
    "#     )\n",
    "\n",
    "\n",
    "# param= {'silent':True,'objective':'binary:logistic',\"eta\":0.4}\n",
    "\n",
    "param= {'eval_metric':'logloss',\"eta\":0.5,}\n",
    "\n",
    "\n",
    "num_round = 30 # n_estimators\n",
    "\n",
    "#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入\n",
    "bst = xgb.train( param, dtrain, num_round )\n",
    "\n",
    "y_pred =( bst.predict(dtest) > 0.5 ).astype(int) #  predict() 返回的是概率  \n",
    "\n",
    "y_true= testLabelArr\n",
    "\n",
    "# 1.正确率\n",
    "print('test dataset accuracy: {} '.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "print('====================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset accuracy: 0.982 \n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 样本不均衡问题\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( '0 负样本所占的比例: {} '.format( len(trainLabelArr[trainLabelArr==0])/len(trainLabelArr) ))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm, accuracy_score as accuracy ,recall_score as recall, roc_auc_score as auc\n",
    "\n",
    "\n",
    "#写明参数\n",
    "scale_pos_weight = [ 0.5 , 1 , 5 , 9 ,10]\n",
    "names = [\n",
    "    \n",
    "         \"negative vs positive: 0.5 \",\n",
    "         \"negative vs positive: 1\",\n",
    "         \"negative vs positive: 5\",\n",
    "         \"negative vs positive: 9\",\n",
    "         \"negative vs positive: 10\"\n",
    "        \n",
    "        ]\n",
    "\n",
    "\n",
    "[*zip(names,scale_pos_weight)]\n",
    "\n",
    "\n",
    "for name,i in zip(names,scale_pos_weight):\n",
    "    \n",
    "    param= { 'eval_metric':'logloss',\"eta\":0.4,\"scale_pos_weight\":i } # scale_pos_weight = 负样本 / 正样本\n",
    "    \n",
    "    num_round = 40\n",
    "    \n",
    "    clf = xgb.train(param, dtrain, num_round)\n",
    "    \n",
    "    preds = clf.predict(dtest)\n",
    "    \n",
    "    ypred = preds.copy()\n",
    "    ypred[preds > 0.5] = 1\n",
    "    ypred[ypred != 1] = 0\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    print(\"\\tAccuracy:{}\".format(accuracy(testLabelArr,ypred)))\n",
    "    print(\"\\tRecall:{}\".format(recall(testLabelArr,ypred)))\n",
    "    print(\"\\tAUC:{}\".format(auc(testLabelArr,preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定参数\n",
    "param1 = { 'eval_metric':'logloss',\"eta\":0.4,\"scale_pos_weight\":9 , \"gamma\":0 }\n",
    "param2 = { 'eval_metric':'logloss',\"eta\":0.4,\"scale_pos_weight\":9 , \"gamma\":5 }\n",
    "\n",
    "num_round = 40\n",
    "n_fold=5 # sklearn - KFold\n",
    "\n",
    "\n",
    "cvresult1 = xgb.cv(param1, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "\n",
    "cvresult2 = xgb.cv(param2, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(range(1,41),cvresult1.iloc[:,0],c=\"red\",label=\"train,gamma=0\")\n",
    "plt.plot(range(1,41),cvresult1.iloc[:,2],c=\"orange\",label=\"test,gamma=0\")\n",
    "plt.plot(range(1,41),cvresult2.iloc[:,0],c=\"green\",label=\"train,gamma=5\")\n",
    "plt.plot(range(1,41),cvresult2.iloc[:,2],c=\"blue\",label=\"test,gamma=5\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#看看类xgb.cv生成了什么结果？\n",
    "\n",
    "cvresult1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higgs 数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载数据集\n",
    "\n",
    "\n",
    "原始 Higgs 数据集\n",
    "ref:\n",
    "https://archive.ics.uci.edu/ml/datasets/HIGGS\n",
    "\n",
    "总记录数\n",
    "11000000\n",
    "\n",
    "\n",
    "论文中使用的大小\n",
    "Higgs 10M( million = 百万) dataset\n",
    "\n",
    "\n",
    "kaggle 竞赛数据集 \n",
    "ref:https://www.kaggle.com/c/higgs-boson/data\n",
    "\n",
    "总记录数\n",
    "550000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 33)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>2.233584</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.347389</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>5.446378</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.245333</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100005</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "      <td>0.083414</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100006</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100007</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "      <td>0.018636</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100008</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.296003</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100009</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>28.859</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>167.735</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "5   100005        89.744                       13.550        59.149   116.344   \n",
       "6   100006       148.754                       28.862       107.782   106.130   \n",
       "7   100007       154.916                       10.418        94.714    29.169   \n",
       "8   100008       105.594                       50.559       100.989     4.288   \n",
       "9   100009       128.053                       88.941        69.272   193.392   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                 0.910           124.711                2.666   \n",
       "1              -999.000          -999.000             -999.000   \n",
       "2              -999.000          -999.000             -999.000   \n",
       "3              -999.000          -999.000             -999.000   \n",
       "4              -999.000          -999.000             -999.000   \n",
       "5                 2.636           284.584               -0.540   \n",
       "6                 0.733           158.359                0.113   \n",
       "7              -999.000          -999.000             -999.000   \n",
       "8              -999.000          -999.000             -999.000   \n",
       "9              -999.000          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  ...  PRI_jet_num  PRI_jet_leading_pt  \\\n",
       "0               3.064      41.928  ...            2              67.435   \n",
       "1               3.473       2.078  ...            1              46.226   \n",
       "2               3.148       9.336  ...            1              44.251   \n",
       "3               3.310       0.414  ...            0            -999.000   \n",
       "4               3.891      16.405  ...            0            -999.000   \n",
       "5               1.362      61.619  ...            3              90.547   \n",
       "6               2.941       2.545  ...            2             123.010   \n",
       "7               2.897       1.526  ...            1              30.638   \n",
       "8               2.904       4.288  ...            0            -999.000   \n",
       "9               1.609      28.859  ...            1             167.735   \n",
       "\n",
       "   PRI_jet_leading_eta  PRI_jet_leading_phi  PRI_jet_subleading_pt  \\\n",
       "0                2.150                0.444                 46.062   \n",
       "1                0.725                1.158               -999.000   \n",
       "2                2.053               -2.028               -999.000   \n",
       "3             -999.000             -999.000               -999.000   \n",
       "4             -999.000             -999.000               -999.000   \n",
       "5               -2.412               -0.653                 56.165   \n",
       "6                0.864                1.450                 56.867   \n",
       "7               -0.715               -1.724               -999.000   \n",
       "8             -999.000             -999.000               -999.000   \n",
       "9               -2.767               -2.514               -999.000   \n",
       "\n",
       "   PRI_jet_subleading_eta  PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  \\\n",
       "0                   1.240                  -2.475         113.497  0.002653   \n",
       "1                -999.000                -999.000          46.226  2.233584   \n",
       "2                -999.000                -999.000          44.251  2.347389   \n",
       "3                -999.000                -999.000          -0.000  5.446378   \n",
       "4                -999.000                -999.000           0.000  6.245333   \n",
       "5                   0.224                   3.106         193.660  0.083414   \n",
       "6                   0.131                  -2.767         179.877  0.002653   \n",
       "7                -999.000                -999.000          30.638  0.018636   \n",
       "8                -999.000                -999.000           0.000  5.296003   \n",
       "9                -999.000                -999.000         167.735  0.001502   \n",
       "\n",
       "   Label  \n",
       "0      s  \n",
       "1      b  \n",
       "2      b  \n",
       "3      b  \n",
       "4      b  \n",
       "5      b  \n",
       "6      s  \n",
       "7      s  \n",
       "8      b  \n",
       "9      s  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Higgs_dataset_path= '../dataset/higgs/kaggle'\n",
    "\n",
    "\n",
    "# 取前10 行 看看长啥样子\n",
    "data = pd.read_csv(Higgs_dataset_path+'/training.csv',skiprows=0,nrows =10 )\n",
    "\n",
    "data.shape\n",
    "data # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading from csv \n"
     ]
    }
   ],
   "source": [
    "# load in training data, directly use numpy\n",
    "dtrain = np.loadtxt(Higgs_dataset_path+'/training.csv' , delimiter=',', skiprows=1, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "\n",
    "# converters 对最后一列进行转换\n",
    "\n",
    "print ('finish loading from csv ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 33)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label  = dtrain[:,32]\n",
    "data   = dtrain[:,1:31]\n",
    "\n",
    "test_size = 550000\n",
    "\n",
    "# rescale weight to make it same as test set\n",
    "weight = dtrain[:,31] * float(test_size) / len(label)\n",
    "\n",
    "sum_wpos = sum( weight[i] for i in range(len(label)) if label[i] == 1.0  )\n",
    "sum_wneg = sum( weight[i] for i in range(len(label)) if label[i] == 0.0  )\n",
    "\n",
    "# print weight statistics\n",
    "print ('weight statistics: wpos=%g, wneg=%g, ratio=%g' % ( sum_wpos, sum_wneg, sum_wneg/sum_wpos ))\n",
    "\n",
    "# construct xgboost.DMatrix from numpy array, treat -999.0 as missing value\n",
    "xgmat = xgb.DMatrix( data, label=label, missing = -999.0, weight=weight )\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use logistic regression loss, use raw prediction before logistic transformation\n",
    "# since we only need the rank\n",
    "param['objective'] = 'binary:logitraw'\n",
    "# scale weight of positive examples\n",
    "param['scale_pos_weight'] = sum_wneg/sum_wpos\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['eval_metric'] = 'auc'\n",
    "param['nthread'] = 16\n",
    "\n",
    "# you can directly throw param in, though we want to watch multiple metrics here\n",
    "plst = list(param.items())+[('eval_metric', 'ams@0.15')] # \n",
    "\n",
    "watchlist = [ (xgmat,'train') ]\n",
    "# boost 120 trees\n",
    "num_round = 120\n",
    "print ('loading data end, start to boost trees')\n",
    "bst = xgb.train( plst, xgmat, num_round, watchlist );\n",
    "# save out model\n",
    "bst.save_model('higgs.model')\n",
    "\n",
    "print ('finish training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**watchlist 使用** \n",
    "\n",
    "作用: 在训练的时候 查看模型的训练效果\n",
    "\n",
    "划分20%为验证集 (dval)，准备一个watchlist 给train和validation set ,这样我们能发现每一个round 的验证集预测结果，如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了( early stop )。\n",
    "\n",
    "训练效果的 评价指标 通过参数 'eval_metric' 控制\n",
    "\n",
    "eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "watchlist = [(dtrain,'train'),(dval,'val')]\n",
    "\n",
    "model = xgb.train(params,dtrain,num_boost_round=100,evals = watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model('higgs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取前10 行 看看长啥样子\n",
    "data = pd.read_csv(Higgs_dataset_path+'/test.csv',skiprows=0,nrows =10 )\n",
    "\n",
    "data.shape # 发现测试数据集 没有标签列, 模型预测完测试集后提交到 kaggle 平台验证\n",
    "data # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data in do training\n",
    "train = np.loadtxt(Higgs_dataset_path+'/training.csv', delimiter=',', skiprows=1, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "\n",
    "label  = train[:,32]\n",
    "data   = train[:,1:31]\n",
    "weight = train[:,31]\n",
    "dtrain = xgb.DMatrix( data, label=label, missing = -999.0, weight=weight )\n",
    "param = {'max_depth':6, 'eta':0.1, 'objective':'binary:logitraw', 'nthread':4}\n",
    "num_round = 120\n",
    "\n",
    "print ('running cross validation, with preprocessing function')\n",
    "# define the preprocessing function\n",
    "# used to return the preprocessed training, test data, and parameter\n",
    "# we can use this to do weight rescale, etc.\n",
    "# as a example, we try to set scale_pos_weight\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label==1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    wtrain *= sum_weight / sum(wtrain)\n",
    "    wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return (dtrain, dtest, param)\n",
    "\n",
    "# do cross validation, for each fold\n",
    "# the dtrain, dtest, param will be passed into fpreproc\n",
    "# then the return value of fpreproc will be used to generate\n",
    "# results of that fold\n",
    "\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,metrics={'ams@0.15', 'auc'}, seed = 0, fpreproc = fpreproc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 近似算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定参数\n",
    "param1 = { 'objective':'binary:logistic',\"eta\":0.1,\"max_depth\":6 , \"nthread\":16}\n",
    "param2 = { 'objective':'binary:logistic',\"eta\":0.1,\"max_depth\":6 , \"nthread\":16, \"tree_method\": 'approx', \"sketch_eps\":0.3}\n",
    "\n",
    "num_round = 90\n",
    "n_fold=5 # sklearn - KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cvresult1 = xgb.cv(param1, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "# 2min 18s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cvresult2 = xgb.cv(param2, dtrain, num_round ,n_fold ,  metrics='auc')\n",
    "\n",
    "# 1min 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(range(1,91),cvresult1.iloc[:,2],c=\"orange\",label=\"test,exact greedy\")\n",
    "plt.plot(range(1,91),cvresult2.iloc[:,2],c=\"blue\",label=\"test,global eps=0.3\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集的技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score , train_test_split \n",
    "\n",
    "\n",
    "data = np.loadtxt(Higgs_dataset_path+'/training.csv' , delimiter=',', skiprows=1,max_rows=10000, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "\n",
    "# max_rows 设置读取的行数\n",
    "# converters 对最后一列进行转换\n",
    "\n",
    "X  = data[:,1:31]\n",
    "y  = data[:,32]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**划分数据集**\n",
    "\n",
    "划分数据集为 训练集 验证集 和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "dtrain = xgb.DMatrix( X_train,label=y_train)\n",
    "dval = xgb.DMatrix( X_val,label=y_val)\n",
    "dtest = xgb.DMatrix( X_test,label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = { 'objective':'binary:logistic',\"eta\":0.1,\"max_depth\":3 , \"nthread\":16}\n",
    "\n",
    "watchlist = [(dtrain,'train'),(dval,'val')]\n",
    "\n",
    "# watchlist = [(dtrain,'train')]\n",
    "\n",
    "param1['eval_metric'] = 'auc'\n",
    "\n",
    "num_round = 120\n",
    "\n",
    "bst = xgb.train( param1 ,dtrain  ,num_round, evals =watchlist )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=bst.predict(dtest)\n",
    "\n",
    "ypred = preds.copy()\n",
    "ypred[preds > 0.5] = 1\n",
    "ypred[ypred != 1] = 0\n",
    "\n",
    "print(\"\\tAccuracy:{}\".format(accuracy(y_test,ypred)))\n",
    "print(\"\\tAUC:{}\".format(auc(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**划分数据集**\n",
    "\n",
    "分层抽样\n",
    "\n",
    "ref:https://blog.csdn.net/haoji007/article/details/106165488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(y[y ==1]) / len(y) # 原数据集的 正样本比例\n",
    "\n",
    "len(y_train[y_train ==1]) / len(y_train)# 训练数据集的 正样本比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "print(split )       \n",
    " \n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[y ==1]) / len(y) # 原数据集的 正样本比例\n",
    "\n",
    "len(y_train[y_train ==1]) / len(y_train)# 训练数据集的 正样本比例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏感知\n",
    "\n",
    "详见 Kaggle_Allstate_Claim_Prediction_Challenge.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MQ2008 数据集 描述\n",
    "\n",
    "每一行是一个查询文档对。第一列是 这个文档对的相关性的标签，第二列是查询id，下面的列是特性，行结尾是关于 文档对的注释，包括文档id。相关性标签越大，查询文档对越相关。查询文档对由46维的特征向量表示。以下是MQ2007数据集中的几个示例行:\n",
    "\n",
    "=================================\n",
    "\n",
    "2 qid:10032 1:0.056537 2:0.000000 3:0.666667 4:1.000000 5:0.067138 … 45:0.000000 46:0.076923 #docid = GX029-35-5894638 inc = 0.0119881192468859 prob = 0.139842\n",
    "\n",
    "0 qid:10032 1:0.279152 2:0.000000 3:0.000000 4:0.000000 5:0.279152 … 45:0.250000 46:1.000000 #docid = GX030-77-6315042 inc = 1 prob = 0.341364\n",
    "\n",
    "0 qid:10032 1:0.130742 2:0.000000 3:0.333333 4:0.000000 5:0.134276 … 45:0.750000 46:1.000000 #docid = GX140-98-13566007 inc = 1 prob = 0.0701303\n",
    "\n",
    "1 qid:10032 1:0.593640 2:1.000000 3:0.000000 4:0.000000 5:0.600707 … 45:0.500000 46:0.000000 #docid = GX256-43-0740276 inc = 0.0136292023050293 prob = 0.400738\n",
    "\n",
    "=================================\n",
    "\n",
    "\n",
    "ref:\n",
    "https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换\n",
    "\n",
    "import sys\n",
    "\n",
    "def save_data(group_data,output_feature,output_group):\n",
    "    if len(group_data) == 0:\n",
    "        return\n",
    "\n",
    "    output_group.write(str(len(group_data))+\"\\n\")\n",
    "    for data in group_data:\n",
    "\n",
    "        # only include nonzero features\n",
    "        feats = [ p for p in data[2:] if float(p.split(':')[1]) != 0.0 ]\n",
    "        output_feature.write(data[0] + \" \" + \" \".join(feats) + \"\\n\")\n",
    "\n",
    "\n",
    "# 传入参数:\n",
    "# ../../../dataset/MQ2008/Fold1/train.txt\n",
    "# ../../../dataset/MQ2008/mq2008.train\n",
    "# ../../../dataset/MQ2008/mq2008.train.group\n",
    "\n",
    "\n",
    "\n",
    "fi = open('../dataset/MQ2008/Fold1/train.txt')\n",
    "\n",
    "output_feature = open('../dataset/MQ2008/mq2008.train',\"w\")\n",
    "output_group = open('../dataset/MQ2008/mq2008.train.group',\"w\")\n",
    "\n",
    "group_data = []\n",
    "group = \"\"\n",
    "for line in fi:\n",
    "    if not line:\n",
    "        break\n",
    "    if \"#\" in line:\n",
    "        line = line[:line.index(\"#\")]\n",
    "    splits = line.strip().split(\" \")\n",
    "\n",
    "    if splits[1] != group:\n",
    "        save_data(group_data,output_feature,output_group)\n",
    "        group_data = []\n",
    "\n",
    "    group = splits[1]\n",
    "    group_data.append(splits)\n",
    "\n",
    "save_data(group_data,output_feature,output_group)\n",
    "\n",
    "fi.close()\n",
    "output_feature.close()\n",
    "output_group.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对原始 数据集进行处理 生成 特征文件 mq2008.train  和 分组文件 train.group\n",
    "\n",
    "(1) 特征文件 中第一列为 文档对 query 相关度的打分, 第二列开始为 特征id: 特征值 ,特征值为0 的特征被排除, 以下是几个实例行：\n",
    "\n",
    "0 1:0.007477 3:1.000000 5:0.007470 11:0.471076 13:1.000000 15:0.477541 16:0.005120\n",
    "0 1:0.603738 3:1.000000 5:0.603175 13:0.122130 16:0.998377 17:0.375000 18:1.000000\n",
    "0 1:0.214953 5:0.213819 11:0.401330 15:0.402388 16:0.140868 17:1.000000 18:0.285714 19:0.333333 20:0.141484 \n",
    "0 3:1.000000 11:0.458053 13:0.495975 15:0.461687 18:0.571429 19:0.833333 21:0.273864 22:0.148498 29:0.387106 \n",
    "\n",
    "(2) 分组文件 中每一行 代表一个 组, 每一行的数字代表这一组拥有的 样本的个数; (只有同一个 group 中的样本才有排序的意义。对于IR任务来说，不同 query对应不同group。)\n",
    "\n",
    "以下是几个实例行：\n",
    "8  当前group 拥有8个样本\n",
    "8\n",
    "8\n",
    "8\n",
    "8\n",
    "16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-map:0.70906\n",
      "[1]\tvalidation-map:0.72783\n",
      "[2]\tvalidation-map:0.72909\n",
      "[3]\tvalidation-map:0.73380\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "dir='../dataset/MQ2008/'\n",
    "\n",
    "#  This script demonstrate how to do ranking with xgboost.train\n",
    "x_train, y_train = load_svmlight_file(dir+\"mq2008.train\") # load_svmlight_file 载入 libsvm 格式的数据, 并将其转换为  Compressed Sparse Row (CSR) matrix\n",
    "x_valid, y_valid = load_svmlight_file(dir+\"mq2008.vali\")\n",
    "x_test, y_test = load_svmlight_file(dir+\"mq2008.test\")\n",
    "\n",
    "# libsvm 使用的文件格式如下：\n",
    "#\n",
    "#  [label] [index1]:[value1] [index2]:[value2] …\n",
    "#\n",
    "# label  目标值，就是说class（属于哪一类），就是你要分类的种类，通常是一些整数。\n",
    "# index 是有顺序的索引，通常是连续的整数。就是指特征编号，必须按照升序排列\n",
    "# value 就是特征值，用来train的数据，通常是一堆实数组成。\n",
    "\n",
    "\n",
    "# x_train 采用 稀疏编码 (CSR) 进行存储\n",
    "# print(x_train) #  仅仅是 打印出来的样子 , 并不代表实际的存储格式\n",
    "#   (0, 0)\t0.007477\n",
    "#   (0, 2)\t1.0\n",
    "#   (0, 4)\t0.00747\n",
    "#   (0, 10)\t0.471076\n",
    "#   (0, 12)\t1.0\n",
    "#   (0, 14)\t0.477541\n",
    "#   (0, 15)\t0.00512\n",
    "#   (0, 17)\t0.571429\n",
    "\n",
    "\n",
    "\n",
    "group_train = []\n",
    "with open( dir+\"mq2008.train.group\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        group_train.append(int(line.split(\"\\n\")[0]))\n",
    "\n",
    "group_valid = []\n",
    "with open( dir+\"mq2008.vali.group\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        group_valid.append(int(line.split(\"\\n\")[0]))\n",
    "\n",
    "group_test = []\n",
    "with open(dir+\"mq2008.test.group\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        group_test.append( int(line.split(\"\\n\")[0]) )\n",
    "\n",
    "train_dmatrix = DMatrix(x_train, y_train)\n",
    "valid_dmatrix = DMatrix(x_valid, y_valid)\n",
    "test_dmatrix = DMatrix(x_test)\n",
    "\n",
    "# DMatrix有set_group方法，调用设置 groupId。\n",
    "# (groupId 的概念在 rank 中广泛适用，只有同一个 group 中的样本才有排序的意义。对于IR任务来说，不同 query对应不同group。)\n",
    "# 注意set_group 方法传入的是每个 group 中元素的个数，\n",
    "\n",
    "\n",
    "train_dmatrix.set_group(group_train)\n",
    "valid_dmatrix.set_group(group_valid)\n",
    "\n",
    "params = {'objective': 'rank:ndcg', 'eta': 0.1, 'gamma': 1.0,\n",
    "          'min_child_weight': 0.1, 'max_depth': 6}\n",
    "\n",
    "xgb_model = xgb.train(params, train_dmatrix, num_boost_round=4,\n",
    "                      evals=[(valid_dmatrix, 'validation')])\n",
    "\n",
    "pred = xgb_model.predict(test_dmatrix) # 输出对 文档对的打分 , 对这些分值进行排序 即可得到最后的 doc list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78897315, 0.17356825, 0.78815585, ..., 0.3806271 , 0.42701086,\n",
       "       0.17356825], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我的 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [5],\n",
       "       [4]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[6, 0],\n",
       "       [5, 1],\n",
       "       [4, 2]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[4, 2],\n",
       "       [5, 1],\n",
       "       [6, 0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=np.array(range(3)).reshape(-1, 1)\n",
    "f=np.array([6,5,4]).reshape(-1, 1)\n",
    "\n",
    "f\n",
    "idx\n",
    "\n",
    "f_idx=np.concatenate([f,idx],axis=1)\n",
    "\n",
    "f_idx\n",
    "\n",
    "# np.sort(f_idx,axis=0 )\n",
    "# np.sort(f_idx,axis=1 )\n",
    "\n",
    "f_idx = f_idx[f_idx[:,0].argsort()] # 按照第0列 对行排序\n",
    "f_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_idx[:1+1,:]\n",
    "f_idx[1+1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_idx[:2+1,:]\n",
    "f_idx[2+1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left=f_idx[:1+1,:]\n",
    "right=f_idx[1+1:,:]\n",
    "\n",
    "index_left=left[:,1]\n",
    "index_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_k=np.array([[10, 0],\n",
    "                  [11, 1],\n",
    "                  [12, 2]])\n",
    "\n",
    "# block_k[:,1]==[1,2]\n",
    "\n",
    "# block_k[:,1]==[0,1,2]\n",
    "\n",
    "# block_k[ block_k[:,1]==[0,1,2] , : ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition= np.array([ True if sample_id in set(index_left) else False for sample_id in block_k[:,1]  ] )\n",
    "\n",
    "condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  1],\n",
       "       [12,  2]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_k[ condition , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_not= ~condition\n",
    "block_k[ condition_not , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3494850021680094"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ln'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f02ed25772bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[1;32m--> 215\u001b[1;33m                                      \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ln'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "(1-1/np.log2(10))*0.5\n",
    "\n",
    "np.ln(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=2\n",
    "\n",
    "~a\n",
    "\n",
    "~(~a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GradeStats:\n",
    "    \"\"\"\n",
    "    梯度 统计信息\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # 一阶梯度的和\n",
    "        self.sum_grad=0\n",
    "\n",
    "        # 二阶梯度的和\n",
    "        self.sum_hess=0\n",
    "\n",
    "# GradeStats_list = [GradeStats()]*10\n",
    "        \n",
    "GradeStats_list = [GradeStats() for i in range(10)]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    GradeStats_list[i].sum_grad=i\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(GradeStats_list[i].sum_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1799015944768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(GradeStats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a={np.nan, 0}\n",
    "\n",
    "0 in a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNumbers:\n",
    "    \n",
    "  def __iter__(self):\n",
    "    self.a = 1\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    x = self.a\n",
    "    self.a += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "myclass = MyNumbers()\n",
    "myiter = iter(myclass)\n",
    "print(next(myiter)) #1\n",
    "print(next(myiter)) #2\n",
    "print(next(myiter)) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncreIDGenerator:\n",
    "    \"\"\"\n",
    "    自增 id 生成器\n",
    "\n",
    "    eg.\n",
    "    ob = IncreIDGenerator()\n",
    "\n",
    "    next(ob.id) # 0\n",
    "    next(ob.id) # 1\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, start=0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param start: id开始值\n",
    "        \"\"\"\n",
    "\n",
    "        self.id = self.incre(start)\n",
    "\n",
    "    def incre(self, n):\n",
    "        \"\"\"\n",
    "        使用生成器实现\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            yield n\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob = IncreIDGenerator(0)\n",
    "\n",
    "next(ob.id) # 0\n",
    "next(ob.id) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(ob.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=set()\n",
    "\n",
    "a.add(3)\n",
    "a.add(1)\n",
    "a.add(2)\n",
    "\n",
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2147483676&((1<< 31) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54289"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "a=233\n",
    "\n",
    "a**2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
